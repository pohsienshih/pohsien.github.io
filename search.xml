<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[End-to-End Testing for Kubernetes (Part II) - Conformance Testing]]></title>
    <url>%2F2019%2FE2E-Testing-for-Kubernetes-Part-II%2F</url>
    <content type="text"><![CDATA[此系列文為 E2E Testing for Kubernetes 的介紹，如尚未看過前一篇文章，可以參考底下連結: E2E Testing for Kubernetes (Part I) - kubetest E2E Testing for Kubernetes (Part II) - Comformance Testing Conformance Testing在前一篇文章裡，我們介紹到如何使用 kubetest 執行 Kubernetes E2E Test ， kubetest 會使用 Kubernetes Binary 開啟新的乾淨 Cluster 去做測試。在這一篇文章裡我們會介紹如何對自己部署的 Cluster 做 E2E Test。 Conformance Testing ，一致性測試，這個測試是 E2E Testing 底下的一個子集合，其主要是驗證 Kubernetes 核心的功能或是 GA 版本的 API ，也就是不管任何版本的 Cluster 都應該要能使用的基本功能。這個測試也可以解讀成: 「不管任何版本或平台的 Cluster 在經過 Conformance Test 之後的結果都要是一致的」。由於 Conformance Test 比較不限定於 Cluster 版本或是平台，因此可以用來測試自己部署的 Cluster 是否是正常的。另外 Conformance Test 也是非破壞性的測試，不會包含 [Disruptive] 的測項，因此並不會影響到 Cluster 上面運行的其他服務。 在前一篇文章有提到 Kind of Test ，其中一個 Label 是 [Conformance] ，這個就是代表 Conformance Test 的測項。 Conformance Test RequirementsKubernetes 有列出 Conformance Test 需要滿足的條件，有非常多項，其中包含只測試 GA 版本的 API 、必須可以使用在任何 Provider 上、不需要連接到 Public Network 、不需要 Privileged 權限或是不能包含具有關於 Node 或是平台相依性的測試，如硬體規格限制等等。不過這份文件也有提到如果將來有足夠的測項能測試 Cluster 之後，會慢慢的放寬這些條件，加入更多的測項進來。 it tests only GA, non-optional features or APIs (e.g., no alpha or beta endpoints, no feature flags required, no deprecated features) it does not require direct access to kubelet’s API to pass (nor does it require indirect access via the API server node proxy endpoint); it MAY use the kubelet API for debugging purposes upon failure it works for all providers (e.g., no SkipIfProviderIs/SkipUnlessProviderIs calls) it is non-privileged (e.g., does not require root on nodes, access to raw network interfaces, or Cluster admin permissions) it works without access to the public internet (short of whatever is required to pre-pull images for conformance tests)… 如果想了解更細部的需求條件可以參考 Conformance Test Requirements。 Exexcution我們同樣可以使用 kubetest 來執行 Conformance Test ，執行步驟非常簡單，只需要限定只執行 [Conformance] Label 的測項即可。 前置作業 # Checking your kubernetes server version$ kubectl get versionServer Version: version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"clean", BuildDate:"2019-08-19T11:05:50Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125;# Change Kubernetes repository to the specific version$ cd $GOPATH/src/k8s.io/kubernetes$ git checkout v1.15.3# Build e2e.test, ginkgo and kubectl# 這邊也可以直接使用 kubetest --build ，只是 Conformance Testing 不需要 # build 這麼多東西。$ make WHAT="test/e2e/e2e.test vendor/github.com/onsi/ginkgo/ginkgo cmd/kubectl" 這邊需要注意的是: kubetest 會直接抓 Kubernetes 目錄 Build 出來的 kubectl 當作 Client 去做測試， 如果 kubectl 的版本 ( GitVersion ) 與 Server 版本不一致， kubetest 會報錯，因此最好先將 Kubernetes 切換到對應版本的 branch/tag ，再 build e2e.test 跟 kubectl 比較好。 如果真的想要用不同版本的 Client (kubectl) 做測試，可以在 kubetest 後面加上 --check-version-skew=false flag，但是不建議這樣做，因為可能會影響測試的準確度。 執行測試 # Setup for conformance tests$ export KUBECONFIG=/path/to/kubeconfig$ export KUBERNETES_CONFORMANCE_TEST=y# Run testing$ kubetest --provider=skeleton --test --test_args="--ginkgo.focus=\[Conformance\]"# Run testing with different version of client$ kubetest --provider=skeleton --test --test_args="--ginkgo.focus=\[Conformance\]" --check-version-skew=false 當設定 KUBERNETES_CONFORMANCE_TEST=y 時， kubernetes/hack/ginkgo-e2e.sh 就會從 KUBECONFIG 來取得 Master 位置，因此不再需要使用 --deployment flag 去告訴 kubetest (Ginkgo) Cluster 資訊。 --provider=skeleton: 這邊的 skeleton 筆者也不確定如何解釋，類似於只有骨架，表示你的 Cluster 只有提供 API 介面(直接提供kubeconfig就可以存取到 Cluster )，而看不到整個整體的 Cluster ，這邊如果讀者有更好的解釋的話，再麻煩幫我修正一下，謝謝。 筆者實際執行 Conformance Test 大概花了 3~4 小時左右，不過同樣可能跟筆者 Homelab 硬體資源有關，實際上應該不需要這多時間。 Test Lists由於 Conformance Testing 是 E2E Testing 的子集合，因此其測項一樣位於 kubernetes/test/e2e/ 裡，不過 Kubernetes 有特別將 Conformance Testing 的測試項目清單記錄在 conformance.txt 底下，如果要細看每一個測項內容的話也可以參考 Kubernetes Conformance Test Suite - v1.9。 kubernetes/test/conformance/testdata/conformance.txt 查看其中一個 spec ，會發現 Conformance Test 是使用 framework.ConformanceIt() ，而不是像 E2E Test 使用 Ginkgo.It() 。 kubernetes/test/e2e/common/pods.go .../* Release : v1.9 Testname: Pods, assigned hostip Description: Create a Pod. Pod status MUST return successfully and contains a valid IP address.*/framework.ConformanceIt("should get a host IP [NodeConformance]", func() &#123; name := "pod-hostip-" + string(uuid.NewUUID()) testHostIP(podClient, &amp;v1.Pod&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: name, &#125;, Spec: v1.PodSpec&#123; Containers: []v1.Container&#123; &#123; Name: "test", Image: imageutils.GetPauseImageName(), &#125;, &#125;, &#125;, &#125;)&#125;)... 實際去看 framework.ConformanceIt() 內容 (位於 kubernetes/test/e2e/framework/framework.go) ，會發現 ConformanceIt() 的功能為貼 [Conformance] Label ，至於為什麼要特別額外用一個 function 來貼 Label ，這邊官方註解是方便靜態分析用，不過筆者認為這樣也可以方便之後針對 Conformance Test 作修改，例如對所有的 Conformance Test 加上額外的 Label 或是增加什麼內容，直接透過 ConformanceIt() 就可以統一修改了。 kubernetes/test/e2e/framework/framework.go ...// ConformanceIt is wrapper function for ginkgo It. Adds "[Conformance]" tag and makes static analysis easier.func ConformanceIt(text string, body interface&#123;&#125;, timeout ...float64) bool &#123; return ginkgo.It(text+" [Conformance]", body, timeout...)&#125;... Write Your Own Test接下來介紹如何撰寫自己的測項 ，基本上都跟 E2E Test 差不多，不過不確定是不是因為一般使用者比較常使用到 Conformance Test ， 這部分文件比 E2E Test 完整許多。 我們將撰寫測試分成三個步驟： 撰寫測試。 確保你的測試有符合 [Conformance Test Requirements](#Conformance-Test-Requirements）。 發送 PR 。 細部流程可參考 Promoting Tests to Conformance 。 1. 撰寫測試在撰寫 Conformance Test 時必須遵守兩個的格式，一個是一定要使用 framework.ConformanceIt() ，不要使用 Ginkgo.It 。 另一個是一定要撰寫 metadata ，格式如下： /* Release : v1.15.3 Testname: Kubelet: log output Description: By default the stdout and stderr from the process being executed in a pod MUST be sent to the pod's logs.*/framework.ConformanceIt("it should print the output to logs", func() &#123; ...&#125;) 測試放置位置以及 Import 位置都與我們之前撰寫的 E2E Test 相同，因此這邊只簡單列出流程，細節部分可以參考前一篇文章。 1. 在 kubernets/test/e2e 建立資料夾來放置你的測試檔。2. 在 kubernets/test/e2e/e2e_test.go 以及 kubernets/test/e2e/BUILD import 你的測試。3. 在 kubernets/test/e2e/&lt;yourfolder&gt;/ 下新增你的 BUILD 檔。4. 更新 conformance.txt5. 使用 kubetest --build 重新 build e2e.test 比較需要注意是第四步必須執行以下指令來更新 conformance.txt :$ go run test/conformance/walk.go test/e2e &gt; test/conformance/testdata/conformance.txt 請記得在撰寫自己的測試時一定要加上對應 Label ，如 [Slow] 、 [Serial] 、 [Disruptive] 等等，這樣可以讓測試人員更加了解你的測項的特性，如果不確定要加上哪些 Label 可以在 slack 或是 發佈 Issue / PR 時，標註 #kinds-of-tests 來詢問。 2. 確保你的測試有符合 [Conformance Test Requirements]這部分可以參考 Conformance Test Requirements。 3. 發送 PR 1. PR Title: "Promote xxx e2e test to Conformance"2. 撰寫測項的資訊以及 metadata，並加上 PR Label 以及標註負責的 SIG。 - /area conformance - @kubernetes/sig-architecture-pr-reviews @kubernetes/sig-xxx-pr-reviews @kubernetes/cncf-conformance-wg - Any necessary information (e.g. 例如解釋為什麼測項無法在 Windows 執行)3. 將 PR 加到 SIG Architecture's Conformance Test Review board 的 To Triage 欄位。4. 使用 /test pull-kubernetes-e2e-aks-engine-azure-windows 來測試項目能否正常跑在 Windows Node 上 Conformance Test Review board For WindowsComforance Testing 並沒有強制要求測項一定要支援 Windows Node ，但是既然 Kubernetes 有支援 Windows ，那還是必須要能夠對 Windows Node 做 Conformance Test 。在現有的測項中，大部分的測項都是可以在 Windows Node 執行的，只有少部分貼上 [LinuxOnly] 的測項是不能跑在 Windows 上的。 在撰寫自己的測試時，如果不確定 Linux Node 和 Windows Node 在測試執行上的差異或是不確定你寫的測項是否可以運行在 Windows 的話，可以參考 Windows &amp; Linux Considerations 。 如果撰寫的測試不支援 Windows ，一定要標註 [LinuxOnly] Label，且在送PR時寫清楚為何不能跑在 Windows Node 上，讓 Reviewer 知道。 VMware Tanzu (Heptio) Sonobuoy另外一個常見的 Kubernetes Conformance Testing Tool 是由 VMware Tanzu (Heptio) 開發的 Sonobuoy 。 Sonobuoy 提供比 kubetest 更方便的介面來做 Conformance Test ，只需要簡單幾行指令就可以執行測試，測試項目也與 kubetest 使用的 ( kubernetes/test/e2e/... ) 是完全一樣的，差別在於 Sonobuoy 是預先將測項全部放到 Docker Image 裡面，因此不需要像 kubetest 需要特別 Build e2e.test，不過需要注意的是 Sonobuoy 只支援前三新的 Kubernetes 版本。 Sonobuoy Website Certified KubernetesSonobuoy 同時也是 CNCF 官方用來認證 Kubernetes Cluster 的 Conformance Testing Tool，企業可以使用 Sonobuoy 來驗證自己開發的 Kubernetes 部署工具佈出來的 Cluster ，如果通過測試可以將結果提交到 GitHub ，該部署工具就可以獲得 CNCF 的認證標章，並顯示在 CNCF 官網上，詳細驗證流程可以參考 Certified Kubernetes 。 ExecutionSonobuoy 執行方式也很簡單，只需要設定 KUBECONFIG 然後執行 sonobuoy run 就可以跑 Conformance Test 了。$ export KUBECONFIG=/path/to/kubeconfig$ go get -u -v github.com/heptio/sonobuoy# Start testing and wait until they are finished run:$ sonobuoy run --wait# Get the result$ results=$(sonobuoy retrieve)$ sonobuoy e2e $results# Delete the objects deployed by sonobuoy$ sonobuoy delete ResultSonobuoy 執行結果報告大致架構如下( Sonobuoy Snapshot Layout )： servergroups.json: 紀錄 Kubernetes API 資訊serverversion.json: 紀錄 Kubernetes Cluster 版本資訊 hosts放置每個 Node 的 Configuration ( configz.json )跟健康狀態 ( healthz.json ) meta config.json: 放置 Sonobuoy 的設定檔。 query-time.json: Sonobuoy 在處理各種 Resource 的反應時間，類似 Performenace Testing 。 run.log: Sonobuoy執行的 log ，注意這不是 Kubernetes Log。 plugins放置 Plugins 的詳細資訊。 podlogs放置 Pod 的 Log ，注意這裡預設只會有 Sonobuoy 產生的 Pod 的 Log。 resources放置 Kubernetes Cluster 的 Resource 資訊。 Cluster: 放所有不屬於任何 Namespaces 的 Resources，如 RoleBinding 、 Namespaces 等等。ns: 放各個不同 Namespaces 的 Resources。 這邊是依照 Resource 種類去作分類，一個種類就是一個 JSON 檔，例如 pohsien namespace 裡面有100個 Pod ， Sonobuoy 就會在 resources/ns/pohsien/pods.json 放100個 Pod 資訊，並不會有100個 JSON 檔。 Sonobuoy vs kubetestSonobuoy 提供了比 kubetest 更加簡單的方式執行 Conformance Test，其不需要 Build Kubernetes Binary ，也不需要比較麻煩的步驟執行測試。輸出結果部分，Sonobuoy 也提供很完善的 Output 機制，方便測試人員去查看測試結果。因為上述的優點，所以 Sonobuoy 成為了目前主流的 Kubernetes Conformance Testing Tool 。 但是由於 Sonobuoy 畢竟是第三方的工具，它會從 Kubernetes 官方目錄取得測項以及需要的資料並放置在 Sonobuoy Docker Image ，這提升了使用上方便性，但也導致其可能會沒辦法取得最新的測試項目。主要因為 Kubernetes 更新速度非常快，在每一次的更新之後， Sonobuoy 都會需要去更新 Image，而在等待 Sonobuoy 更新這段時間測項就會出現落差。不過筆者認為如果沒有要測試最新版本的 Cluster ，就比較不需要擔心這個問題，或著是可以設定排程一段時間更新 Sonobuoy ，然後再次做測試。 Summary在這系列文我們介紹了 Kubernets End to End Testing ，包括怎使用 kubetest 執行 E2E Test 以及 Conformance Test ，也簡單介紹了如何撰寫自己的測項，最後也介紹如何使用 CNCF 官方的 Comformance Testing Tool - Sonobuoy 。 E2E Testing 是在企業導入 Kubernetes 流程中是相當重要的一步，無論是使用什麼工具部署，建議都使用 E2E Testing 來驗證 Cluster 是否正常，這樣當發生任何問題時，比較可以排除掉 Cluster 本身的問題，減少問題定位的成本。 另外也可以定期的執行 E2E Testing / Conformance Testing 或是直接將測試整合進 CI/CD 流程裡，不過最好根據自己的環境狀況來評估，雖然 Conformance Testing 是無破壞性的測試，但是直接對於 Production 環境去做測試其實還是會有一些風險 (例如影響效能或是非預期內錯誤等等) ，因此還是需要特別注意，或著是也可以在測試前，先將重要服務轉移出去，待測試完成後再轉移回來，也是一個方法，實際執行方式可以根據自己的環境去做評估。 References Conformance Testing in Kubernetes Kubernetes Conformance Test Suite - v1.9 Conformance.txt Sonobuoy CNCF - Certified Kubernetes 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[End-to-End Testing for Kubernetes (Part I) - kubetest]]></title>
    <url>%2F2019%2FE2E-Testing-for-Kubernetes-Part-I%2F</url>
    <content type="text"><![CDATA[現今 Kubernetes Cluster 部署方式越來越多種，部署門檻也越來越低，但是在部署完之後要如何確認自己的 Cluster 真的是正常可用的？ 大多數的人(包含筆者以前)都是簡單部署一些 Deployment 或是 Service 看是否有 running 就認為 Cluster 是正常的，或是部署時沒有看到什麼錯誤訊息就覺得沒事，但是這樣是不太足夠的，因為有可能壞掉的是一些不常使用的功能或是人工比較難已發現的問題，這樣之後如果使用者部署應用時剛好使用到相關功能，發生問題將會很難查找。 為了確認 Cluster 功能是否正常，我們可以針對 Cluster 做完整的測試，測試各個API是否正常，但是筆者查了一下發現網路上比較少關於 Kubernetes 測試的詳細資訊，因此花了一點時間研究並紀錄在此系列文章。 注意， Kubernetes 的測試有非常多種層面，例如： Unit Testing 、 Integration Testing 、 E2E Testing 、 Performance/Benchmark Testing 、 Load Testing 、 Stress Testing 或是 Security Testing，但是在此系列文中，強調的是測試 Cluster 的功能是否正常，因此重點會放在 E2E Testing 上。 E2E Testing for Kubernetes: End-to-End Testing for Kubernetes (Part I) - kubetest End-to-End Testing for Kubernetes (Part II) - Comformance Testing 另外筆者在 Cloud Native Taiwan User Group (CNTUG) Meetup #20 也有一場 talk 是在介紹這個主題，大家可以參考以下的簡報： Software Testing一般 Software Testing 可分成三種比較常見的測試方式: Unit Testing單元測試，測試單一功能或是某個函數，例如測試登入功能是否正常。這種測試維度比較小，測試程式的撰寫也比較單純一點。 Integration Testing整合測試，這種通常是測試多個具有關聯或相依性的功能(或函數)整合之後是否正常，維度比單元測試還要大一點。 End to End (E2E) Testing前面單元測試跟整合測試兩個都是比較偏向 Developer 驗證 Code 或是函數是否正常，而 E2E 測試比較偏向以使用者角度去操作系統，測試人員把自己當成一般使用者去對整個系統做操作，測試每個功能是否正常。 由於我們只是要確認 Kubernetes Cluster 功能是否正常，並不是要開發，因此此系列文會著重在 Kubernetes E2E Testing 。 SIG-TestingSpecial Interest Group (SIG) 是由一群志同道合來自各公司各領域的人組成的組織或是團體，主要是一起學習某個技術、對某個技術進行探討或是一起維護某個專案，類似社群的概念，這些 SIG 可能也會定期的舉辦 Meetup 或是 Conference 等。 Kubernetes Community 是由許多不同的 SIG 組成，例如: sig-network 是負責 Kubernetes 中網路部分、 sig-docs 是負責文件、而 sig-storage 是負責儲存部分等等。每個 SIG 都有自己負責的 Subproject ，這些 Subproject 可能是文件撰寫，也有可能是 Code 撰寫。除了 SIG 之外，Kubnernetes Community還有其他的 Subgroups 如 Committees 、Working Groups 跟 User Groups 等等， 如果對於 Kubernetes Community 組成有興趣，可以參考: Kubernetes Community Governance Model 。如果想了解 Kubernetes 中各個 SIG，可以參考: Kubernetes SIGs and Working Groups。如果想了解各個 SIG 負責的 Subproject ，可以參考: Subprojects of each SIGs。 在眾多 SIG 當中，負責 Kubernetes 測試的是 sig-testing ， sig-testing 負責許多知名的 subprojects ，如管理 Kubernetes project 的 CI/CD 系統 - prow ，以及部署 K8s in docker 的 kind 等等。在這裡我們主要會提到的是 - test-infra 。 test-infra 提供了許多供 Kubernetes 測試的工具，裡面包含了我們要介紹的 Kubernetes E2E Testing Tool - kubetest。 Cluster E2E TestingKubernetes E2E Testing 主要是驗證所有的功能 (包含 API Server 以及 Controller ) 是否是正常可用的，且 API 行為必須跟 Spec 上一樣。透過這種測試能夠找出一些 Unit Test 、 Integration Test 或是人工難以找出的問題。 Kubetest Usagetest-infra 釋出了可以對 Kubernetes Cluster 做 E2E Test 的工具 - kubetest ，不過 kubetest 其實是一個整合的介面，他不只可以做一般的 E2E Test ，還整合了如 Conformance Test, Node E2E Test 以及 Performance Test 等等的測試，後續我們會再細部介紹 kubetest 的運作流程。 這邊需要注意的是: kubetest 在執行 E2E Test 時，會開啟一個乾淨的 Cluster 來做測試，這樣的用意是因為 Kubernetes 是想要針對整個 Kubernetes Source Code 做 E2E Test ，這樣才可以確保當前 Release 出來的 Kubernetes 版本是正常可用的。 因此 kubetest 在執行時會去尋找 Kubernetes Repository ，找到後再 Build Source Code ，接著開啟新的 Cluster 測試。 kubetest 其實也有被整合到 prow 裡面，任何 Pull Requet 在被 Merge 前，也都會透過 kubetest 進行測試。 Execution:$ kubetest --build --provider &lt;yourprovider&gt; --deployment &lt;yourdeployer&gt; --up --test --test_args="--ginkgo.skip(focus)=xxx" --dump &lt;folder&gt; --down Flag:--build: Build binaries (e2e.test, e2e_node.test) for testing.--provider: Specify an alternative provider (gce, local, gke, aks, etc.) for E2E testing, default value is gce.--deployment: Deployment strategies of Kubernetes cluster. (gce, local, gke, aks, etc.)--up: Turn up a new cluster.--test: Run E2E testing.--test_args: Test matching for ginkgo.--down: Shutdown and delete the cluster--dump: Export the result. ( junit xml format ) 這邊比較容易搞混的是 --provider 以及 --deployment ，官方文件對這兩個 flag 並沒有太多的解釋，經過筆者研究了之後，整理出來結論是: --provider 比較像是告訴 kubetest 要在哪個平台上做測試或著是告訴 kubetest 準備哪一個平台使用的 Test Lists，而 --deployment 比較像是告訴 kubetest 你的 Cluster 位置，讓 kubetest 可以存取到 Cluster 。舉例來說如果你設定 --deployment gke ， kubetest 就會知道你的 Cluster 是用 gke 佈出來的，接下來就會等你再提供一些 gke 相關參數(例如提供 kubeconfig 或是一些 token 之類的)，讓 kubetest 能夠存取得到你的 Cluster 。需要注意的是 --provider 跟 --deployment 兩個平台必須一致，否則 kubetest 會無法測試並且報錯，例如 --provider local 配上 --deployment gke 或是 --provider gke 配 --deployment local 這種情況是不行的。 筆者實際執行完整的 E2E Test 大概花了一天左右，不過可能跟筆者 Homelab 硬體資源有關，實際上應該不需要這多時間。 使用 --dump flag 會將測試結果存成 JUnit 格式，裡面會寫所有執行的測項以及測試結果 (包括失敗訊息): Kubetest Workflow接下來解釋一下 kubetest 運作流程，到底執行 kubetest 之後它做哪些事情呢？ 上圖為 kubetest 簡單的執行流程圖，基本上大致流程是 kubernetes/hack/e2e.go -&gt; test-infra/kubetest -&gt; kubernetes/hack/ginkgo-e2e.sh -&gt; kubernetes/test/e2e/e2e_test.go -&gt; kubernetes/test/e2e/framework/framework.go -&gt; Start E2E testing 。 Stage 1: kubernetes/hack/e2e.go -&gt; test-infra/kubetestkubernetes/hack/e2e.go在以往還沒有 kubetest 的時候， Kubernetes 測試都是各自分開的介面，而其中 E2E Testing 部分就是 kubernetes/hack/e2e.go 負責，但是 sig-testing 可能是為了整合這些介面，因此開發了 kubetest ，來讓測試人員能夠直接使用單一介面來做到各式各樣的測試( Cluster E2E Test 、 Node E2E Test 或 Performance Test 等等)。如果你直接去看 kubernetes/hack/e2e.go 的 Source Code，你會發現其實它也是會去抓 kubetest 並且執行 kubetest ，所以你可能會看到網路上一些文章介紹 E2E Testing 的時候是執行 hack/e2e.go --provider xxx ... 而不是 kubetest --provider ... ，這兩個其實是一樣的。 如果你去看 kubernetes/hack/e2e.go 的 history ，可以看到他在2017年的時候把 Code 整個改成 kubetest 介面，由此可以得知是從那個時候開始整合進去的。 Stage 2: test-infra/kubetest -&gt; kubernetes/hack/ginkgo-e2e.shGinkgo &amp; GomegaKubernetes E2E Testing 所有測項其實都是用 Ginkgo 以及 Gomega 撰寫的， Ginkgo 是 Golang 的 Behavior-Driven Development (BDD) Testing Framework ，而 Gomaga 是 Golang 的 Matcher Library， 用在測項結果比對。 這邊簡單解釋一下 BDD 為何，一般在做 Software Testing 時， RD 或是 QA 可能會依照規格書撰寫出對應的測試 Code ，但是這些 Code 可能只有技術人員看得懂，這樣會導致技術與非技術人員很難協同作業或討論，因此很容易發生開發者誤解測項或是規格書制定不清楚的情況。而 BDD 就是為了解決這個問題，讓開發人員以及非技術人員能夠一同協作的一種開發方式，首先大家會共同制定一份規格書，規格書會使用更接近人類語意的自然語言來描述軟體功能和測試案例，制定完後， QA 能夠直接執行這份規格書來測試，無需再額外寫一些複雜的測試 Code，簡單來說 BDD 就是以軟體的行為來描述測試，讓大家都能看懂。 Ginkgo 主要由以下三個部分組成： Main Program: 主要要測的程式。 Test Suite: 測試包，通常會包含很多 Spec (測項)。 Spec: 測項 (可能一個或多個)。 以下使用簡單的範例來介紹如何使用 Ginkgo 和 Gomaga ，範例 Code 可以從筆者 GitHub 上下載，有興趣可以去載下來執行看看。 下載並安裝 Ginkgo 和 Gomega $ go get github.com/onsi/ginkgo/ginkgo$ go get github.com/onsi/gomega/...# Install Ginkgo CLI$ go install github.com/onsi/ginkgo/ginkgo 這邊撰寫一個簡單的 myproject package ，裡面會有兩個函數: Greeting() 和 Calc() 。 Greeting() 功能是是傳入一個字串，會回傳 Hello xxx 的打招呼函數； Calc() 是傳入一個數值，會回傳十倍的值給你。myproject.go package myprojectfunc Greeting(name string) string&#123; return "Hello " + name + "."&#125;func Calc(number int) int&#123; return number * 10&#125; 新增 Test Suite ， Test Suite 就是測試包的意思，測試包會去抓所有的測試檔，並執行裡面的測項 ( Spec )。 $ cd myproject$ lsmyproject.go$ ginkgo bootstrap$ lsmyproject.go myproject_suite_test.go 查看 Test Suite ，會發現裡面會有一個 Testxxx() 函數。預設 Golang 本身就有支援測試的功能 ( go test )，當執行 go test 後， go 會去抓目錄底下所有 xxx_test.go 的檔案，並執行這些 go 檔裡面 Testxxx() 的函數，這些函數就是你寫的測項。 Ginkgo 也是使用 go test 這種特性，讓 go 先去執行 Testxxx()，只不過在這個Testxxx() 函數裡執行的是 Ginkgo 的函數 RunSpecs() ， RunSpecs() 會去抓目錄底下所有的測項 ( Spec ) 。myproject_suite_test.go package myproject_test import ( "testing" . "github.com/onsi/ginkgo" . "github.com/onsi/gomega")func TestMyproject(t *testing.T) &#123; RegisterFailHandler(Fail) RunSpecs(t, "Myproject Suite")&#125; 接著就是產生測試檔 $ ginkgo generate mytest$ lsmyproject.go myproject_suite_test.go mytest_test.go mytest_test.go package myproject_test import ( . "github.com/onsi/ginkgo" . "github.com/onsi/gomega" . "github.com/myproject")var _ = Describe("Mytest", func() &#123;&#125;) 撰寫測項 ( Spec ) ，如果你曾經有寫過其他語言的測試，你可能會發現 Ginkgo 架構與他們非常相似，都是由三個部分組成: Describe , Context 以及 It ， Describe 是描述這個 Spec ，例如: 測試 Greeting() 函數 。 Context 是補充說明這個 Spec ，可以增加條件式或是任何規則還讓 Spec 行為更加明確，例如： 傳給它一個字串 或是 傳給它一個中英文夾雜的字串 等等。最後 It 是預期達到的結果，例如： 應該要跟我打招呼 , 要回傳什麼值 或 要報錯 等等。 在此範例中， Spec 1 是測試傳入一個字串給 Greeing() ，驗證是否會回傳 Hello &lt;字串&gt;。 Spec 2 是測試傳入一個數值給 Calc() ，驗證是否會回傳十倍的值。 mytest_test.go ...var _ = Describe("Mytest", func() &#123; var name string= "test" var number int= 99 // Spec 1 Describe("Test Greeting function", func() &#123; Context("Giva a name", func() &#123; It("Should greeting", func() &#123; Expect(Greeting(name)).To(Equal("Hello "+name+".")) &#125;) &#125;) &#125;) // Spec 2 Describe("Test Calc function", func() &#123; Context("Give a number", func() &#123; It("Should get the correct result", func() &#123; Expect(Calc(number)).To(Equal(number*10)) &#125;) &#125;) &#125;)&#125;) 另一個範例是 Ginkgo 的一個函數 BeforeEach() ，由於 Kubernetes 的 E2E Test 中使用了很多 BeforeEach ，因此這裡特別解釋一下 BeforeEach 的作用。 BeforeEach 執行時機是在 每個 Spec 執行前 ，我們可以將一些參數初始化的步驟或是一些可能會被 Spec 影響的步驟放到 BeforeEach 裡面，這樣可以確保每次的測試都是公平乾淨且不受影響的。 從底下例子來看，我們宣告一個 number 並給它值 99 ， Spec 1 會去執行驗證 Calc() 函數，驗證完後會把 number 值改掉，如果沒有把 number = 99 放到 BeforeEach() 裡面，那 Spec 2 就會直接使用 被改過 的 number ( 這時應該是 990 ) 去做測試，然後就會測失敗，因此必須將 number = 99 放到 BeforeEach() 裡面，讓 Ginkgo 執行每個 Spec 之前都重新指派一次 number 的值。 mytest2_test.go … var _ = Describe("Mytest", func() &#123; var number int // 如果將 number 指派放到 BeforeEach 外面，則 number 的值會被 Spec 1 改掉。 // number = 99 BeforeEach(func()&#123; // 將number 指派放到 BeforeEach 裡面可以確保值永遠都是 99 number = 99 &#125;) Describe("Test Calc function again", func() &#123; Context("Give a number 99", func() &#123; // Spec 1 It("Should return 990", func() &#123; Expect(Calc(number)).To(Equal(990)) number = Calc(number2) &#125;) // Spec 2 It("Should return 990, too", func() &#123; Expect(Calc(number)).To(Equal(990)) &#125;) &#125;) &#125;)&#125;) 執行測試，我們可以直接使用 ginkgo 或是 go test 指令來執行測試。執行完後 Ginkgo 會顯示執行了幾個 Spec ，然後幾個成功幾個失敗。 $ ginkgo# Show all Specs$ ginkgo -vRunning Suite: Myproject Suite==============================Random Seed: 1568041760Will run 4 of 4 Specs•••Ran 4 of 4 Specs in 0.001 secondsSUCCESS! -- 4 Passed | 0 Failed | 0 Pending | 0 SkippedPASSGinkgo ran 1 suite in 3.471827325sTest Suite Passed 到這邊大概了解 Ginkgo 的作用後，接下來我們回到 test-infra/kubetest ，根據程式碼來看一下 kubetest 做的事。 當我們執行 kubetest --test 的時候， test-infra/kubetest/main.go 會去執行 complete() 函數，接著根據指定的 --deployment 執行 run(deploy, *o)。 test-infra/kubetest/main.go ...func main() &#123; ... err := complete(o) ...&#125;func complete(o *options) error &#123; ... if err := run(deploy, *o); err != nil &#123; return err &#125; ...&#125;... test-infra/kubetest/e2e.go 裡的 run() 函數，會判斷是否有設定 --test flag，有得話就執行 Run() 來做 E2E test ，可以看到 Run() 函數裡會去呼叫 ginkgo-e2e.sh 。 test-infra/kubetest/e2e.go ...func run(deploy deployer, o options) error &#123; ... // 判斷是否有設定 --test flag if o.test &#123; // 判斷是否有 build 了 e2e.test if err := control.XMLWrap(&amp;suite, "test setup", deploy.TestSetup); err != nil &#123; ... &#125; ... else &#123; if o.deployment != "conformance" &#123; ... &#125; ... else&#123; ... if tester != nil &#123; // 開始測試 return tester.Run(control, testArgs); &#125; ... &#125; &#125; &#125; &#125;...// Run executes ./hack/ginkgo-e2e.shfunc (t *GinkgoScriptTester) Run(control *process.Control, testArgs []string) error &#123; return control.FinishRunning(exec.Command("./hack/ginkgo-e2e.sh", testArgs...))&#125; 由此可以知道 kubetest 其實是使用 Ginkgo 來執行所有測項，並且會執行 ginkgo-e2e.sh。 Stage 3: kubernetes/hack/ginkgo-e2e.sh -&gt; kubernetes/test/e2e/e2e_test.go -&gt; kubernetes/test/e2e/framework/framework.go在 Stage 2 我們已經知道 kubetest 會執行 Ginkgo ， 接下來解釋到底 Test Lists 在哪裡。我們實際去看 kubernetes/hack/ginkgo-e2e.sh 原始碼，會看到 Ginkgo 會去執行 e2e.test。 kubernetes/hack/ginkgo-e2e.sh...# Find the ginkgo binary build as part of the release.ginkgo=$(kube::util::find-binary "ginkgo")e2e_test=$(kube::util::find-binary "e2e.test")..."$&#123;ginkgo&#125;" "$&#123;ginkgo_args[@]:+$&#123;ginkgo_args[@]&#125;&#125;" "$&#123;e2e_test&#125;" -- \ "$&#123;auth_config[@]:+$&#123;auth_config[@]&#125;&#125;" \ --ginkgo.flakeAttempts="$&#123;FLAKE_ATTEMPTS&#125;" \... e2e.test 就是 Test Suite 以及 測項的 Binary 檔，前面我們有提到 kubetest --build 會去 Build Kubernetes Source Code ，其中一個就是把 kubernetes/test/e2e Build 成 e2e.test (可參考 BUILD)。 這邊 Build 其實就是做 make 的動作，如果對於 make 細節有興趣，可以去研究 Kubernetes 目錄裡的 Makefile 。 我們再回到 kubetest 原始碼來看實際步驟，當執行 kubetest --build 時，test-infra/kubetest/main.go 會去執行 Build() 函數，而這個 BUild() 函數是位於 test-infra/kubetest/build.go 裡。 test-infra/kubetest/main.gofunc acquireKubernetes(o *options, d deployer) error &#123; // Potentially build kubernetes // 判斷有沒有設定 --build flag if o.build.Enabled() &#123; var err error // kind deployer manages build if k, ok := d.(*kind.Deployer); ok &#123; err = control.XMLWrap(&amp;suite, "Build", k.Build) &#125; else &#123; // o.build.Build 就是 kubetest --build="值" 指定的值，如果沒指定 // 預設是 quick-release err = control.XMLWrap(&amp;suite, "Build", o.build.Build) &#125; ... &#125;&#125; Build() 函數會實際去執行 make 來 Build Source Code 。 test-infra/kubetest/build.gofunc (b *buildStrategy) Build() error &#123; var target string switch *b &#123; case "bazel": target = "bazel-release" case "e2e": //TODO(Q-Lee): we should have a better way of build just the e2e tests target = "bazel-release" ... case "host-go": target = "all" case "quick": target = "quick-release" case "release": target = "release" case "gce-windows-bazel": ... default: return fmt.Errorf("Unknown build strategy: %v", b) &#125; ... // 執行 make -C kubernetes &lt;target&gt; return control.FinishRunning(exec.Command("make", "-C", util.K8s("kubernetes"), target))&#125; 到這邊我們已經知道 kubetest --build 執行細節以及 e2e.test 的由來，接下來我們再討論 e2e.test 裡的 Test Suite 以及 Spec 到底是什麼？ Test Suite前面有提到 Golang 在執行 go test 時會去找 xxx_test.go 的檔案，並且執行裡面的 Testxxx() 函數。因此當 Ginkgo 執行 e2e.test 時，會找到 kubernetes/test/e2e/e2e_test.go 檔，然後去執行 kubernetes/test/e2e/e2e.go 裡面的 TestE2E() 函數。 kubernetes/test/e2e/e2e_test.gofunc TestE2E(t *testing.T) &#123; RunE2ETests(t)&#125; TestE2E() 函數會執行 Ginkgo 函數 RunSpec() 來執行所有 Spec，由此可以知道 e2e_test.go 以及 e2e.go 就是 Test Suite 。 kubernetes/test/e2e/e2e.gofunc RunE2ETests(t *testing.T) &#123; ... ginkgo.RunSpecsWithDefaultAndCustomReporters(t, "Kubernetes e2e suite", r)&#125; Specs一般來說使用 Ginkgo 來做測試，為了方便 Test Suite 去抓到所有的 Spec ，會在 Test Suite 檔案 Import Spec 的 {ackage (除非 Spec 與 Test Suite 同個 Package )，所以如果直接從 e2e_test.go 以及 e2e.go 去看，就會發現在 e2e_test.go 裡 Import 了所有 E2E Test 用到的 Spec ，可以發現這些測項就是位於 kubernetes/test/e2e/ 底下。 kubernetes/test/e2e/e2e_test.gopackage e2eimport ( "flag" ... // test sources _ "k8s.io/kubernetes/test/e2e/apimachinery" _ "k8s.io/kubernetes/test/e2e/apps" _ "k8s.io/kubernetes/test/e2e/auth" _ "k8s.io/kubernetes/test/e2e/autoscaling" _ "k8s.io/kubernetes/test/e2e/cloud" _ "k8s.io/kubernetes/test/e2e/common" _ "k8s.io/kubernetes/test/e2e/instrumentation" _ "k8s.io/kubernetes/test/e2e/kubectl" _ "k8s.io/kubernetes/test/e2e/lifecycle" _ "k8s.io/kubernetes/test/e2e/lifecycle/bootstrap" _ "k8s.io/kubernetes/test/e2e/network" _ "k8s.io/kubernetes/test/e2e/node" _ "k8s.io/kubernetes/test/e2e/scheduling" _ "k8s.io/kubernetes/test/e2e/servicecatalog" _ "k8s.io/kubernetes/test/e2e/storage" _ "k8s.io/kubernetes/test/e2e/storage/external" _ "k8s.io/kubernetes/test/e2e/ui" _ "k8s.io/kubernetes/test/e2e/windows")... 從裡面挑了一個 Spec 來看，可以看到 Spec 格式就是使用 Ginkgo 以及 Gomega 寫出來的。 kubernetes/test/e2e/ui/dashboard.gopackage ui...var _ = SIGDescribe("Kubernetes Dashboard [Feature:Dashboard]", func() &#123; ginkgo.BeforeEach(func() &#123; // TODO(kubernetes/kubernetes#61559): Enable dashboard here rather than skip the test. framework.SkipIfProviderIs("gke") &#125;) const ( uiServiceName = "kubernetes-dashboard" uiAppName = uiServiceName uiNamespace = metav1.NamespaceSystem serverStartTimeout = 1 * time.Minute ) f := framework.NewDefaultFramework(uiServiceName) ginkgo.It("should check that the kubernetes-dashboard instance is alive", func() &#123; ginkgo.By("Checking whether the kubernetes-dashboard service exists.") err := framework.WaitForService(f.ClientSet, uiNamespace, uiServiceName, true, framework.Poll, framework.ServiceStartTimeout) framework.ExpectNoError(err) ginkgo.By("Checking to make sure the kubernetes-dashboard pods are running") selector := Labels.SelectorFromSet(Labels.Set(map[string]string&#123;"k8s-app": uiAppName&#125;)) err = testutils.WaitForPodsWithLabelRunning(f.ClientSet, uiNamespace, selector) framework.ExpectNoError(err) ginkgo.By("Checking to make sure we get a response from the kubernetes-dashboard.") err = wait.Poll(framework.Poll, serverStartTimeout, func() (bool, error) &#123; var status int proxyRequest, errProxy := e2eservice.GetServicesProxyRequest(f.ClientSet, f.ClientSet.CoreV1().RESTClient().Get()) if errProxy != nil &#123; framework.Logf("Get services proxy request failed: %v", errProxy) &#125; ctx, cancel := context.WithTimeout(context.Background(), framework.SingleCallTimeout) defer cancel() // Query against the proxy URL for the kubernetes-dashboard service. err := proxyRequest.Namespace(uiNamespace). Context(ctx). Name(utilnet.JoinSchemeNamePort("https", uiServiceName, "")). Timeout(framework.SingleCallTimeout). Do(). StatusCode(&amp;status). Error() if err != nil &#123; if ctx.Err() != nil &#123; framework.Failf("Request to kubernetes-dashboard failed: %v", err) return true, err &#125; framework.Logf("Request to kubernetes-dashboard failed: %v", err) &#125; else if status != http.StatusOK &#123; framework.Logf("Unexpected status from kubernetes-dashboard: %v", status) &#125; // Don't return err here as it aborts polling. return status == http.StatusOK, nil &#125;) framework.ExpectNoError(err) &#125;)&#125;) 此 Spec 是檢查 kubernetes dashboard 是否正常執行 Kind of Tests從前面小節我們得知 Kubernetes E2E Testing 的測項，但是這些測項非常的多，要如何區別這些測項呢? Kubernetes 針對這部份採用了賦予 Label 的方式來區別不同種類的測項。 [Slow] - 執行時間超過兩分鐘的測項。 [Serial] - 需要依序執行，而不能平行執行的測項。 [Disruptive] - 具有破壞性或是會影響其他測試的測項，例如重開機 node ，或是砍掉 kube-system 相關的 pod 等等。 [Internet] - 會需要連到外部網路的測項。 [Conformance] - Conformace Testing 的測項。 [LinuxOnly] - 只能跑在 Linux node 的測項。 [Privileged] - 會需要 privileged container 的測項。 [Alpha] - 測試 Alpha 功能的測項。… 詳細 Label 資訊可以參考 - Kinds of tests 一個測項 ( Spec ) 可以貼上一個或是多個 Label ，貼的位置可以在 SIGDescribe 或是 Ginkgo.It ，如以下範例：ginkgo.It("should provide Internet connection for containers [Feature:Networking-IPv6][Experimental][LinuxOnly]", func() &#123; // IPv6 is not supported on Windows. framework.SkipIfNodeOSDistroIs("windows") ginkgo.By("Running container which tries to connect to 2001:4860:4860::8888") framework.ExpectNoError( framework.CheckConnectivityToHost(f, "", "connectivity-test", "2001:4860:4860::8888", 53, 30))&#125;)... var _ = SIGDescribe("Kubernetes Dashboard [Feature:Dashboard]", func() &#123; ginkgo.BeforeEach(func() &#123; framework.SkipIfProviderIs("gke") &#125;)... Execute Specific Kind of Tests在 kubetest 使用上，要過濾或是執行特定的測項，只需要使用 --test_args flag 然後裡面再設定 --ginkgo.focus/skip 就可以了。--test_args 是讓 kubetest 可以使用 Ginkgo CLI 的 flag ，因此不單單可以用 focus/skip ，只要是 Ginkgo CLI 支援的 flag ，基本上都可以使用。focus/skip 可以透過正規表示式來比對 Describe 或是 ginkgo.It 描述裡的 Label。 # Only execute tests with LinuxOnly Label.$ kubetest --test --test_args="--ginkgo.focus=\[LinuxOnly\]" --provider local --deployment local# Skip the tests with LinuxOnly Label.$ kubetest --test --test_args="--ginkgo.skip=\[LinuxOnly\]" --provider local --deployment local focus/skip 可以用來比對 Describe 或是 ginkgo.It 描述裡的 Label。 framework.gokubernetes/test/e2e/framework/framework.go 是用來執行一些 E2E test 會使用到的函數，如後續會提到的 Conformance Testing 就是在 framework.go 裡執行 framework.ConformanceIt() 來做貼 Label 的動作。其餘 framework.go 細節筆者就沒深入去瞭解，因此這部分就不深入解釋。 Write Your Own Test在了解 kubetest 流程之後，接下來我們來試著撰寫自己的測試。 SpecificationsKubernetes Community 在 Writing good e2e tests for Kubernetes 裡提到一些撰寫測試前需要注意的事項以及一些規範，大致為以下幾項： Debuggability : 在寫測試時盡量將所有訊息寫清楚，例如測試失敗時，失敗訊息要寫明確說是哪裡錯誤，不要就含糊的顯示「測試失敗」之類的訊息，必須讓測試人員容易去 Debug 。 Ability to run in non-dedicated test clusters : 不要限定測試只能跑在特定的 Cluster，這裡的意思是撰寫測試時不要有假設的情況，任何測項都要寫清楚。例如：假設這個 Cluster 是乾淨的，上面沒有跑任何服務、假設環境裡已經有 xxx 服務或是假設 Cluster 跑在什麼硬體、環境等等。這裡官方舉了一個例子: 假如今天寫了一個測試「確認你的 Pod 能跑在所有 Node 上」來驗證所有 Node 有沒有問題，這個測項看起來似乎沒問題，但是實際上做了一個「同時間內只有你這個測項跑在 Node 上或是同時間沒有其他服務在 Node 上」的假設。假如你在跑這項測試前， Cluster 正在同時跑其他測試(或是服務)，導致某個 Node 資源被佔滿或是執行到 [Disruptive] 的測項，這樣這個測試就會被影響而失敗，但是他的失敗並不是 Node 本身有問題，而是被其他東西影響，這樣測試出來就會不準確。 另外是盡量避免撰寫 [Disruptive] 的測項，我們前面有提到 [Disruptive] 是具破壞性的測項，可能會影響其他測試，例如：重開機、刪掉服務等等。這裡避免的意思一樣是不要假設：「同時間只有你這個測項在測試」，避免影響到其他測試或服務，如果真的要撰寫該類型的測試，一定要加 [Disruptive] Label，並且把測項描述寫清楚。 最後是盡量不要使用非 Kubernetes 官方的 API 來做測試，因為這樣測試失敗時很難找出是 API 問題還是 Cluster 問題。 Speed of execution : 在寫測試時，盡量提升測試的效率、壓低測試的時間，不要放一些如 sleep 這種耗時間的函數，如果測試時間會超過兩分鐘以上就加上 [Slow] Label，讓測試者知道這個是比較耗時的測項。 另外除了測試花的時間之外，還必須設定好測試失敗的時間，例如：你的測試很簡單只需要兩分鐘內完成，但是可能光是等 Pod Ready 就等超過兩分鐘甚至是 Pod 已經卡住了，如果沒有設定測試失敗的時間 (如10分鐘後就認定測試失敗) ，這個測試就會永遠卡在那。 Resilience to relatively rare, temporary infrastructure glitches or delays : 在寫測試時，要彈性一點，當遇到失敗的情況，多試幾次，有可能只是剛好一些情況導致失敗。舉例來說：你的測項是 「測試 Cluster 能不能跑起來 Nginx 的 Pod 」，有可能第一次在測剛好網路比較不穩， Image 抓比較慢或是抓不下來，導致測試失敗，但是第二次網路就恢復重抓就抓下來了，因此不要在第一次失敗就直接認定測試失敗，過個幾秒再重抓 Image 一次，測試就會正常了。 這裡需要注意的是，雖然說要給測項一些彈性，不過也是要根據你的測項來判斷適不適用。 Add New Test了解測試撰寫規範之後，接下來就來實際撰寫測試，以下範例可以從我的 GitHub - kubernetes-e2e-practice 裡取得，有興趣者，可以載下來測試。 該範例適用於 Kubernetes v1.15.x 版本 在這個範例裡， 我們會撰寫一個小測試驗證是否能在 Cluster 部署一個名為 pohsien 的 Pod ，並確認這個 Pod 是否成功執行起來。 首先在 kubernetes/test/e2e 新增一個 pohsien 資料夾，裡面就是放置我們自己撰寫的測試檔。 $ mkdir -c kubernetes/test/e2e/pohsien 接著撰寫自己的測試，並放置在剛剛建立的 kubernetes/test/e2e/pohsien/ 裡面，基本上這邊都是使用 client-go 來操作 Kubernetes 資源。我們定義一個 [Pohsien] Label ，然後把我們的測項貼上這個 Label ，以方便之後執行測試。 $ vim kubernetes/test/e2e/pohsien/pohsien.go$ vim kubernetes/test/e2e/pohsien/framework.go pohsien.go package pohsien...var _ = SIGDescribe("Kubernetes Pohsien's Pod [Pohsien]", func() &#123; f := framework.NewDefaultFramework("pods") var podClient *framework.PodClient ginkgo.BeforeEach(func() &#123; podClient = f.PodClient() &#125;) ginkgo.It("Make sure the pohsien pod can be deployed", func()&#123; // 建立 pod ginkgo.By("Create Pod") pod := &amp;corev1.Pod&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: "pohsien", Labels: map[string]string&#123; "name": "pohsien", &#125;, &#125;, Spec: corev1.PodSpec&#123; Containers: []corev1.Container&#123; &#123; Name: "nginx", Image: "nginx:1.17.3", &#125;, &#125;, &#125;, &#125; podClient.Create(pod) // 檢查 pod 是否成功運行起來 ginkgo.By("Get the pod") podGetting, err := podClient.Get(pod.Name, metav1.GetOptions&#123;&#125;) framework.ExpectNoError(err, "Failed to get the pod") framework.ExpectNoError(f.WaitForPodRunning(podGetting.Name)) gomega.Expect(podGetting.Name, "pohsien") &#125;)&#125;) framework.go 是用來宣告 SIGDescribe() 函數，而其他的測試檔會呼叫這個函數並且傳遞相關參數(例如要設定的 Label 以及 Spec 等等），一般來說會在 framework.go 設定一些 Global 的設定或變數，例如幫整個該資料夾的測項貼上對應的 SIG Label。雖然 framework.go 應該不是必要的，但是為了與其他測項一致，因此在此範例我們也新增了 framework.go ，並且加上了我們虛構的 [pohsien-testing] Label 。 framework.go package pohsienimport "github.com/onsi/ginkgo"// SIGDescribe annotates the test with the SIG Label.func SIGDescribe(text string, body func()) bool &#123; return ginkgo.Describe("[pohsien-testing] "+text, body)&#125; 由於我們範例只是 Demo 用，因此在這裡並沒有特別嚴謹的指定 Label ，但請記得在撰寫自己的測試時一定要加上對應 Label ，如 [Slow] 、 [Serial] 、 [Disruptive] 等等，這樣可以讓測試人員更加了解你的測項的特性，如果不確定要加上哪些 Label 可以在 slack 或是 發佈 Issue / PR 時，標註 #kinds-of-tests 來詢問。 接下來在 kubernetes/test/e2e/e2e_test.go 裡面 Import 我們自己的測項。framework.go package e2eimport ( "flag" "fmt" ... // test sources _ "k8s.io/kubernetes/test/e2e/apimachinery" _ "k8s.io/kubernetes/test/e2e/apps" ... _ "k8s.io/kubernetes/test/e2e/pohsien")... 接下來撰寫及修改 BUILD 檔來讓 kubetest --build 或是 make 時能抓到我們寫的測試檔。主要有兩個地方: 一個是在 kubernetes/test/e2e/pohsien/ 底下新增 BUILD 檔，另一個是在 kubernetes/test/e2e/BUILD 裡加上我們寫的測試檔。 BUILD 檔寫法可以參考其他 E2E 測項。 $ vim kubernetes/test/e2e/pohsien/BUILD$ vim kubernetes/test/e2e/BUILD 完成後，接下來我們就可以執行看看。 $ kubetest --build$ kubetest --test --test_args="--ginkgo.focus=\[Pohsien\]"[pohsien-testing] Kubernetes Pohsien's Pod [Pohsien]Make sure the pohsien pod can be deployed/root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pohsien/pohsien.go:35...STEP: Create PodSTEP: Get the pod[AfterEach] [pohsien-testing] Kubernetes Pohsien's Pod [Pohsien] /root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151Oct 25 09:33:54.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be readySTEP: Destroying namespace "pods-1457" for this suite.Oct 25 09:34:16.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discoveredOct 25 09:34:16.740: INFO: namespace pods-1457 deletion completed in 22.085125288sOct 25 09:34:16.742: INFO: Running AfterSuite actions on all nodesOct 25 09:34:16.742: INFO: Running AfterSuite actions on node 1Ran 1 of 4414 Specs in 28.369 secondsSUCCESS! -- 1 Passed | 0 Failed | 0 Pending | 4413 SkippedPASSGinkgo ran 1 suite in 29.513161526sTest Suite Passed2019/10/25 09:34:16 process.go:155: Step './hack/ginkgo-e2e.sh --ginkgo.focus=\[Pohsien\]' finished in 29.787466628s 到這裡 Kubernetes E2E Testing 解釋就告一段落，下一篇文章會介紹如何在自己建立的 Cluster 做 E2E Test 。 References Types Of Software Testing: Different Testing Types With Details WIKIPEDIA - Special Interest Group Kubernetes SIGs and Working Groups Kubernetes Community Governance Model Subprojects of each SIGs Sig-Testing test-infra Kubernetes E2E Testing Kubetest Ginkgo Gomega End-To-End Testing in Kubernetes Writing good e2e tests for Kubernetes 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK Filebeat With Log Rotate]]></title>
    <url>%2F2019%2FELK-Filebeat-With-Log-Rotate%2F</url>
    <content type="text"><![CDATA[Log Rotate意思是當log隨著時間擴大後，能夠自動打包壓縮或是清除掉舊log，以免系統空間被log吃光。 ELK Filebeat並無支援log rotate的功能，Beats主要功能是做log shipping，不包含log rotate，因此需要透過額外的方式做。 在此文章中測試環境為 Ubuntu Server 18.04 LTS 方法1: logrotate Tool網路上查到大部分幾乎都是採用Linux內建的logrotate工具來配合Filebeat，一般Linux系統的套件如syslog, apt-get等等其實都已經有使用logrotate這個工具來做管理，這個工具可以設定定期或是根據大小來做rotate。 以下為一個小範例: 在/var/log底下建立一個pohsien.log。 $ cd /var/log; vim pohsien.log$ ls -l pohsien.log-rw-r--r-- 1 root root 204 Aug 20 09:47 pohsien.log 將/var/log/pohsien.log加到logrotate設定檔裡面。 $ vim /etc/logrotate.conf 新增以下內容 /var/log/pohsien.log&#123; # 大小超過500KB就執行log rotate maxsize 500K # 將舊的檔案壓縮，並創建新的pohsien.log檔 # 如果沒壓縮的話，舊檔會改名，然後一樣會創建新的pohsien.log檔 compress # 做超過兩次log rotate就把最舊的資料砍掉，也就是最多只會留兩個壓縮檔 rotate 2&#125; 測試log rotate 重複寫入pohsien.log然後手動執行logrotate，可以發現當檔案超過500KB，就會自動產生pohsien.log.x.gz，且最多只會有兩個壓縮檔，最舊的會一直被砍掉。 手動執行logrotate $ logrotate -v /etc/logrotate.conf 查看結果 $ ls -l /var/log 注意事項在此範例中只有使用到一些功能，實際上logrotate可用的功能相當多，可視各種情境做調整。 另外logrotate是使用cronjob執行的，在Ubuntu預設是15分鐘執行一次，如果想要自訂執行時間可以自己另外寫cronjob執行logrotate，另外要注意的是如果要跟filebeat一起使用，需要配合好兩個的執行時間。 參考資料 鳥哥的Linux私房菜 Logrotate - http://linux.vbird.org/linux_basic/0570syslog.php#rotate How to Use logrotate to Manage Log Files - https://www.linode.com/docs/uptime/logs/use-logrotate-to-manage-log-files/ 相關討論 Log rotation and filebeat - https://discuss.elastic.co/t/log-rotation-and-filebeat/140285 方法2: 檢查filebeat registryElasticsearch staff在討論區 文章1跟文章2有提到一個比較tricky的方式: filebeat每次成功傳送完檔案後會把記錄寫在/var/lib/filebeat/registry/filebeat/data.json 寫一隻cronjob或是其他程式檢查data.json裡的offset是否跟log檔案大小一樣，如果一樣就代表傳輸完了，然後刪掉傳輸完的log檔案。 相關討論 Delete processed log entries - https://discuss.elastic.co/t/delete-processed-log-entries/75960 FileBeats -Are there any ways we can delete the log files after file beat harvest the data to logstash - https://discuss.elastic.co/t/filebeats-are-there-any-ways-we-can-delete-the-log-files-after-file-beat-harvest-the-data-to-logstash/177997 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>log</tag>
        <tag>system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iThome Kubernetes Summit 2019心得]]></title>
    <url>%2F2019%2F2019-iThome-Kubernetes-Summit-Notes%2F</url>
    <content type="text"><![CDATA[這次是第一次參加iThome Kubernetes Summit，當初看到早鳥票釋出就直接買了($2400左右)，以一天的Conference來說不算便宜，但是能夠去學習學習增廣見聞也是值得的。 結果買票之後才得知我們社群Cloud Native Taiwan User Group(CNTUG)也有去擺攤，雖然是付費票，但是還是去支援了一下社群攤位xD。 上午場都在跟附近社群交流、逛攤位以及顧社群攤位，因此只聽了第一場「賣K8s 的人不敢告訴你的事」。下午花比較多的時間在聽議程，以下列出部分議程的簡單心得跟筆記。 目前官方已經有將簡報釋出，也有釋出當天大會各議程共筆，有興趣的人可以參考： 議程: https://summit.ithome.com.tw/kubernetes/大會共筆: https://hackmd.io/@k8ssummit/19/%2FRe80SUVITjWAMbUwaUVnWw 賣 K8s 的人不敢告訴你的事葉秉哲（William） / 新加坡商鈦坦科技 Technical CoachSlide 這場議程大多比較偏向使用者在導入Kubernetes時需要思考的事，大部分賣K8s平台的廠商在協助導入Kubernetes的時候都只提到好處(好管理Container以及好Scale)，但是都沒有提到維運成本以及其他層面(Networking, Storage etc)。 講者提到了在導入Kubernetes時，可以使用三種方式來更加了解及定位出自己的需求或目標： 1. 任何事情不能只看表面，要了解現實面。使用5W1H(What,Where,When,Why,Who,How)分析法去了解事情，要深入的去了解事情，不能只單單看表面，講者這裡提到一些例子： Kubernetes官方對Kubernetes的介紹：OpenSource、Auto-Deployment、Containerized等等。這個時候可以去思考： OpenSource如果出問題，有哪個廠商可以support嘛？ 為什麼直到現在才有這種Auto-Deployment Container服務或著是跟現有的解決方案差別在哪？ 我們真的做好自動化的準備嗎？我們的Service真的準備好Auto-Deployment了嗎？ 為什麼會需要Containerized？ Service Containerized之後穩定嗎？ … iThome 遍地開花的K8s與容器應用文章，裡面提到K8s、容器化、微服務、部署跟管理方便以及Hybrid Cloud Solution等等。這個時候可以去思考： 為什麼需要微服務? 優缺點是什麼？ 什麼情況下會需要Hybrid Cloud Solution，以及為什麼以前沒有推這種Solution? … Kubernetes at GitHub，GitHub在自家服務使用Kubernetes的成功案例，他們一開始先對github.com以及api.github.com導入Kubernetes，且第一階段先把stateless服務遷移到Kubernetes上。先選擇github.com以及api.github.com的原因是因為這兩個服務導入最艱難，只要這兩個導入成功的，其他的應該就比較沒問題。導入的工作主要是由是SRE、Platform以及Developer三個一起參與導入。看到這些可以去思考： 為什麼導入要先以最艱難的服務導入？不是通常都會先用簡單或比較不重要的服務測試Kubernetes，然後再慢慢把重要的遷移上去嗎？GitHub這樣是有不同考量嗎？對我的企業而言哪種比較適合呢？ 為什麼會需要SRE、Platform以及Developer三種工程師一起參與？缺少任一個可以嗎？ 為何先挑選stateless服務做處理呢？ 2. 擴展自己的知識多擴展自己的知識，可以多看CNCF Cloud Native Interactive Landscape，瞭解project的分類，並且定位自己目前需要達成的是哪些目標，多了解一些，不要冒然選廠商跟解決方案。 另外也可以參考CNCF Trail Map，裡面有提供十個階段來建議企業漸進式導入Cloud Native專案，可以詢問自己目前想要做的是哪一階段或是廠商提供的是哪一階段的服務？ 3. Dev跟Ops都需要了解要導入Kubernetes絕對不會只是Operation的事，Development也是相當重要的，善用各方面的知識，並且互相交流，才能夠降低導入Kubernetes的門檻。 Docker Swarm 太陽春 K8s 太複雜？試試輕量級的 K3s 吧！王偉任（Weithenn） / Micron Technology Senior System Engineer 這個議程主要是在介紹Rancher K3s，提到K3s比起一般Kubernetes還要更加的輕量化及差異點(SQLite取代etcd,預設Container Runtime使用containerd)。由於他非常的輕量，因此很適合部署在如樹莓派這樣的IoT裝置上，且部署方式非常方便，也能夠整合Kubernetes預設的dashboard或是Rancher Dashboard來做管理。 Harden Your Kubernetes Cluster蔡宗城（smalltown） / Maicoin Senior Site Reliability Engineerslide 此議程講者主要針對不同的層面來探討如何增強Kubernetes的安全性。講者提到了非常多實用的經驗跟工具，這裡有些部分沒有筆記到或是不清楚，建議可以直接看簡報比較詳細。 Image 盡量使用Minimal Base Image，太大包的image可能會有不必要的套件，增加安全風險。 限制執行Container的使用者身份，以免讓Container取得過大的權限。 Image盡量不要使用latest版本，因為latest對應到的Image可能會一直變動。 不要隨意相信Registry上的Image，因為你無法確保那真的是官方釋出的。 可以使用簽章機制來識別官方釋出的Image，(可使用Notary)。 Image不要儲存一些敏感資訊，盡量用一些Secret形式來取代。 Use COPY instead of ADD 可使用 Anchor, Clair, Trivy來對image做CVE掃描，找出Image淺在的漏洞。 Regular Vulnerability Assessment -&gt; 評估跟分類掃瞄出來的漏洞 DevSecOps -&gt; 把image security 整合CI/CD。 Credential Kubernetes Secret放在Cluster裡不一定安全。 HashiCorp Vault -&gt; 管理credentials 生命週期及儲存，支援Dynamic Credentials，可以整合public cloud或是DB。 Credentials Lifecycle：不要把Credentials 存在Disk，可存在Memory，使用完就直接清掉，不要保存。 Network 預設情況下，Pod之間是沒有隔離的，網路都是互通。 Kubernetes Network Policy(須注意並不是每個CNI都有支援，slide裡連結有支援表可以參考) Pod跟Pod之間傳輸是明碼的，很容易會有MITM。 Istio 裡面pod跟pod之間會有envoy proxy做傳輸，可以在這邊做手腳(Istio TLS)，將pod跟pod之間傳輸做加密。 Istio with Kiali ( 可以視覺化pod之間pod traffic流量，也會顯示連線是否有加密。 Runtime(pod底層)分成兩個部分 Auditing &amp; Enforcement Enforcement -&gt; 強迫去做一些事情(pod security policy, aurmor) Falco -&gt; 限定使用者不能做什麼事，例如使用者想要exec到pod執行bash，動作就會被擋住，然後Falco會傳通知給administrator。 Policy as Code Open Policy Agent (Kubernetes Admission Control, HTTP API Authorization) OPA Flow，有點類似會有一個authorization/authentication server去對權限做管理，k8s在執行任何動作前都會去問authorization server來決定可不可以執行。 需注意OPA不是使用yaml來撰寫，是自己的語法。 前方有雷別再踩─企業導入 Kubernetes 的掃雷指南朱培華 / 多奇數位創意系統工程師 此議程的講者整理了非常多的Kubernetes Best Practice，從update, scaling, secret管理到label應用、資源應用以及監控等等，非常多實用的內容，這部分因為一些原因沒有記錄太多筆記，不過講者的簡報相當詳細，因此建議可以直接參考簡報。 總結這裡筆者只列出一些議程筆記，其他比較偏安裝教學或是有些沒有筆記到的就沒有寫在這上面，有興趣者可以參考官方的共筆或是簡報。 這次參加的感想是議程部分比較入門類，偏向經驗分享或是工具使用教學，很適合像筆者這樣的菜鳥新手去聽。攤位部分不多，大概只有幾家比較大的贊助商來擺攤，不過廠商贈品送的還蠻多的，另外這次很特別的是有來自國外的Dr.Brad Topol簽書會(書名Kubernetes in the Enterprise)，只要用名片就可以換到一本原文簽名書，算是這次Conference中最佛心的活動。 最後還是覺得票價不便宜，希望明年有機會能夠當講者，除了能累積經驗外，也能使用講者票參加，一舉數得。 \Cloud Native Taiwan User Group攤位 Kubernetes in the Enterprise作者簽書會 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>conference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[針對Enterprise PKS環境的K8s Cluster Node硬碟空間不足測試及解法]]></title>
    <url>%2F2019%2FEnterprise-PKS-K8s-Node-Disk-Out-Of-Space%2F</url>
    <content type="text"><![CDATA[這篇主要是測試Enterprise PKS佈出來的Kubernetes Node 上面的硬碟滿了會造成什麼影響以及要如何解決。其實這個問題應該跟一般Kubernetes環境差不多，只是目前有一些case緣故還是需要在Enterprise PKS環境下測試一下。因此特別針對這部分做測試。 Kubernetes Node上的disk種類在PCF Ops Manager Web GUI &gt; Enterprise PKS Tile &gt; Plan裡，可以設定Enterprise PKS部署出來的Kubernetes Cluster資源，其中有一些欄位是設定node VM的硬碟資訊: 預設BOSH產生出來的node上面會有三顆虛擬硬碟。這裡我們用sda,sdb,sdc來代稱。 sda: Node System Disk，掛載目錄為/，為node VM作業系統的根目錄，這個大小目前是不能設定的，預設3GB。 sdb: Agent Package Disk，掛載目錄為/var/vcap/data...，為存放pks-system以及bosh agent這些必要套件的位置，這個就是PCF Ops Manager上設定的VM Type裡面的disk大小。 sdc: Persistent Disk，掛載目錄/var/vcap/store/...，這個disk在master中是用來存etcd的，因此只要disk沒有掛掉，不管BOSH怎麼修復master，etcd資料都不會消失。如果是worker的話，這個disk是用來存docker container 以及docker image。sdc大小是根據PCF Ops Manager裡面設定的Persistent Disk Type來配置。 當node發生錯誤而啟動BOSH自動修復機制時，只有sdc資料會被保留，其他的都會被清空還原成初始狀態。 上面disk名稱只是方便識別，並非官方取名。 由於disk裡的資料可能會隨著時間成長，例如:log, docker image/containe或是一些暫存檔等等。為了確定清楚硬碟塞爆是否會影響Kubernetes服務，因此接下來會測試塞爆三種disk並觀察會發生的情況。 使用到的指令建立任意大小的檔案$ fallocate -l &lt;檔案大小&gt; &lt;檔案名稱&gt;Example:$ fallocate -l 50G test 查看目錄屬於哪顆disk、大小及使用率$ df &lt;目錄&gt; -hExample:$ df /var/vcap/ -h 查看目錄或檔案大小$ du -sh &lt;檔案名稱/目錄&gt;Example:$ du -sh /var/vcap/ 查看docker根目錄$ docker system info 查看docker image/container數量/大小資訊$ docker system df Node System Disk(sda)測試環境: 1台master, 2台worker 測試結果 Container不會掛掉，正常運行。 BOSH ssh到worker的時候會顯示no space left on device錯誤。 修復方式 關閉壞掉的node讓BOSH自動修復，產生新的node VM。 如果還能登入node，可以清掉一些資料釋出Node System Disk(sda)空間。 測試流程 使用kubectl建立deployment到cluster上。 連到worker VM裡並塞滿sda空間 # SSHbosh -d &lt;deployment&gt; ssh &lt;worker&gt;# 切換到/dev/sda1掛載目錄cd /var# 塞滿空間fallocate -l 10G test Container依然正常運行無異常，但是當執行bosh ssh時出現下圖空間不足錯誤。 Agent Packages Disk(sdb)測試環境: 1台master, 2台worker 測試結果 Node會掛掉，無法負荷原本的container，因此原本上面跑的container都會變成Evicted狀態。(可能會造成服務中斷) 如果有額外的node可用，master會佈新的container到別台node上。如果沒有，所有container都會變成Pending狀態。 修復方式: 清出Agent Packages Disk空間，然後執行monit restart all，這樣node就會恢復正常。 測試流程 使用kubectl建立deployment到cluster上。 連到worker VM(b685858b...)裡並塞滿sdb空間 # SSHbosh -d &lt;deployment&gt; ssh &lt;worker&gt;# 切換到/dev/sdb2掛載目錄cd /var/vcap/data/# 塞滿空間fallocate -l 10G test 使用kubectl查看pods狀態，可以看到剛剛被塞爆的worker(b685858b...)上的pods都進入Evicted狀態，且master在其他node產生新的pods。 使用kubectl describe pods &lt;pod name&gt;查看Evicted pod，可以看到是因為node空間不足導致的。 Persistent Disk(sdc)測試環境: 1台master, 2台worker 測試結果 與Agent Packages Disk(sdb)結果相同 Node會掛掉，無法負荷原本的container，因此原本上面跑的container都會變成Evicted狀態。(可能會造成服務中斷) 如果有額外的node可用，master會佈新的container到別台node上。如果沒有，所有container都會變成Pending狀態。 修復方式: 清出persistent disk空間，然後在node上執行monit restart all指令，這樣node就會恢復正常。 測試流程 使用kubectl建立deployment到cluster上。 連到worker VM(3f28d443...)裡並塞滿sdb空間 # SSHbosh -d &lt;deployment&gt; ssh &lt;worker&gt;# 切換到/dev/sdc1掛載目錄cd /var/vcap/store/docker# 塞滿空間fallocate -l 50G test 使用kubectl查看pods狀態，可以看到剛剛被塞爆的worker(3f28d443...)上的pods都進入Evicted狀態，且master在其他node產生新的pods。 擴大Node Disk大小看到上述測試可以得知disk如果滿了是有可能會影響服務，解決方法除了清掉不必要的檔案釋出空間之外，也可以選擇擴大原有的空間，但目前能擴大的只有sdb以及sdc。 官方提供的擴大方式是到PCF Ops Manager去調原plan設定，例如把disk調大資源調多等等, 然後apply change的時候勾選upgrade all cluster, 這樣原有的cluster就會套用新的設定(cpu/memory/persistent disk…)，但只能修改原plan數值, 無法修改成不同plan(例如從plan1改成plan2)。詳情可參考連結 ，另外此方法的缺點是要改的話，全部同樣使用該plan的cluster都會一併修改。 除了官方解法外，筆者我想到另外一種比較暴力的方法是直接在vCenter把node的硬碟擴大, 但是實際測試這種方式會導致BOSH那邊不同步引發錯誤，因此目前這是不可行的。 以下是兩種方法的測試流程: PCF Ops Manager調整Plan設定 假設目前worker VM (91997fd5...) persistent disk已經被塞爆(預設50G)，希望能擴大空間。 到PCF Ops Manager Web GUI &gt; Enterprise PKS Tile &gt; Plan將worker的persistent disk調整成75G。 Apply Change 完成後連到worker VM(91997fd5...)去看，可以看到空間已經成功擴大。 Pod也可以正常的部署在worker VM (91997fd5...)上了。 從vCenter強制調整Node Disk大小 注意這種方式會導致BOSH那邊不同步引發錯誤，因此目前此方法是不可行的，但還是將過程列出來供參考。 假設目前worker VM (3f28d443...) persistent disk已經被塞爆(預設50G)，希望能擴大空間。 到vCenter上直接去將worker VM persistent disk調整為80G。 接下來為了避免BOSH將這台worker砍掉，因此要先關閉自動修復機制。 $ bosh -d &lt;deployment&gt; update-resurrection off 接下來將worker VM(3f28d443...)關機。 可以從bosh vms看到該台worker1 已經變成unresponsive agent狀態，但是BOSH不會將它砍掉。 接下來到vCenter &gt; Datastore &gt; pcf_disks找到worker VM(3f28d443...)的persistent disk，並擴大它的空間。 先找出worker1對應的persistent disk(Disk CIDs) $ bosh instances -i 到vCenter &gt; Datastore &gt; pcf_disks找到對應的disk，可以發現disk還是50G。 點擊inflate來擴大硬碟。(這個動作會跑一段時間) 好了之後可以看到disk已經變80G。 接下來將worker1開機，但是開好之後還是unresponsive agent。 不得已情況下只好將自動修復開起來，看看BOSH會不會砍掉建立新的worker，然後將persistent disk掛載上去。 $ bosh -d &lt;deployment&gt; update-resurrection on BOSH開始修復，但是修復到最後失敗，硬碟無法掛載，可能是因為disk裡的partition有變動到。到這裡就卡住了，BOSH就無法修復了，也許是某一步做錯或是BOSH本身不支援這種改法，後續如果有成功再來更新此篇，目前暫不將此方法列入解法。 Summary測試下來的感覺好像跟是不是Enterprise PKS環境沒啥關係xD。不過經過此測試也比較清楚Enterprise PKS運行機制以及針對這種情況的處理方式，但對目前的Enterprise PKS來說，擴大硬碟的機制還是很不彈性，這有可能是受到BOSH的限制，希望未來能夠再加強這部分。 Reference PKS Plan Resizing - https://community.pivotal.io/s/question/0D50e00005j9wr2/pks-plan-resizing Changing the thick or thin provisioning of a virtual disk - https://kb.vmware.com/s/article/2014832 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Pivotal Container Service</category>
      </categories>
      <tags>
        <tag>pks</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enterprise PKS介紹]]></title>
    <url>%2F2019%2FEnterprise-PKS-Introduction%2F</url>
    <content type="text"><![CDATA[PKS全名是Pivotal Container Service，顧名思義是由Pivotal這間公司開發的。Pivotal是VMware的子公司，也許比較少人知道這間公司，不過相信大家應該都聽過tomcat或是redis，這兩個知名的專案就是由Pivotal開發的，由此可知Pivotal也是一家技術非常扎實強大的公司。 PKS介紹PKS是用來管理/部署Kubernetes Cluster的一個平台，由於目前Kubernetes在建置或維運上還是需要蠻高的成本及技術門檻，為了要降低維運成本，讓企業能完全專注在自己的服務上，因此才漸漸開始有了這類管理/部署Kubernetes的平台出現。透過這種平台能夠輕鬆的部署Kubernetes Cluster，有些平台甚至還針對Kubernetes Cluster提供HA、Fault Tolerance或是Load Balancing的機制，大大降低維運成本，而PKS就是其中一種這種平台(產品)，類似的其他產品還有Rancher、Redhat OpenShift以及IBM Cloud Private等。目前Pivotal有三種不同的PKS產品： VMware Enterprise PKS VMware Essential PKS VMware Cloud PKS VMware Enterprise PKS，目前為1.4.1版，在1.4.0以前名稱還叫做PKS而已，可能是因為要跟其他產品區分開來因而改名，這個產品架構主要是由Cloud Foundry開發的BOSH專案以及Pivotal各式產品組成，目前支援的平台為vSphere,GCP,Azure以及AWS，本篇會以Enterprise PKS介紹為主，稍後會有較詳細的架構介紹。 VMware Essential PKS是今年2019年3月VMware出的一種新的PKS版本，這個版本比較特殊一點，主要是heptio公司(已被VMware買下來)開發的，因此他的架構主要都是由heptio所提供的Open Source套件組合而成的，例如用來備份Kubernetes的velero、或是作為proxy的contour等等。這個產品目前筆者也搞不清楚定位在哪，感覺與Pivotal開發的Enterprise PKS有較勁意味(Pivotal官方文件也完全沒有提到這個產品，都只有在VMware網站才有xD)，不過目前關於這產品的資訊還太少，也許過一段時間就會知道定位跟差異了。 Cloud PKS是VMware提供的一個Public Cloud Kubernetes環境，是一個SaaS平台，使用者可以直接將Container部署上去，完全不需要維護Kubernetes環境，詳細資訊可以參考連結。 PKS架構組成 圖片為簡略的Enterprise PKS架構圖，由於Pivotal是VMware的子公司，因此目前Enterprise PKS對vSphere環境相容性最好，尤其是結合VMwaer NSX-T能夠完全發揮出Enterprise PKS功能。不過不管是運行在哪個平台，Enterprise PKS元件都會以VM的形式運行，因此是不會與平台中其他一般VM相衝突的。 Enterprise PKS1. PCF Ops Manager這是Pivotal的一個產品，主要是提供一個GUI平台讓使用者可以直接上傳Pivotal系列產品來進行部署。Enterprise PKS也是透過這種方式安裝。Pivotal的產品是用Tile為單位(.pivotal檔)，只需要先將Tile上傳到Ops Manager，再手動設定一些參數(IP,密碼等等)，就可以直接按下變更鈕部署，非常方便。如果要刪掉產品也只要先按Tile旁邊的垃圾桶，再案變更鈕即可。 如下圖所示，PCF Ops Manager上面有三個Tile，分別是BOSH Tile,PKS Tile以及Harbor Tile。 PCF Ops Manager只是一個Tile管控介面，它一定要結合BOSH(下一點會提到)才能部署，所以實際上當使用者按下變更鈕之後應該是Ops Manager會呼叫BOSH Director去根據設定的參數部署這些Tile。 Pivotal官網有釋出許多它們包好的Tile，不過大部分都是他們自家的產品，是要付費的。如果想要自己做Tile，可以參照連結將自己的服務打包成Tile來安裝。 2. BOSH DirectorBOSH是整個Enterprise PKS的核心，BOSH是由Cloud Foundry開發的Open Source專案，用來部署、監控、修復VM。而Cloud Foundry本身平台也是採用BOSH來建置的。在PKS當中，所有的元件包含後面提到的PKS API Server、Harbor或K8s Cluster都是由BOSH Director部署的，BOSH Director還會定期地去監控所有元件的健康狀態(每幾秒就polling一次)，只要發生有問題的元件，就會啟動自動修復機制，非常方便。 3. PKS API ServerPKS API Server負責建置Kubernetes Cluster，不過這是比較攏統的說法，其實是PKS API Server收到Request之後，會通知BOSH Director部署K8s Cluster的Node(VM)，等到Node(VM)都建好後，PKS API Server就會開始針對這些VM去做設定及安裝一些PKS及Kubernetes的必要元件，如kube-proxy, kubelet, docker daemon等等。 4. Kubernetes Cluster分成master以及worker，目前PKS佈出來的Kubernetes cluster都是佈在VM上面(畢竟一開始本來就是為vSphere打造的)。PKS可以一次部署多個Cluster，因此企業可以根據不同部門，給不同的Kubernetes Cluster並且指派不同的權限，且彼此是獨立開來的。所有透過BOSH Director跟PKS API Server部署出來的VM，上面都會預先裝好BOSH Agent以及PKS Service(Agent)，BOSH就是透過這些Agent來設定VM以及監控VM健康狀態。 另外目前Enterprise PKS只支援Docker Container，不確定之後會不會整合其他的Container Runtime。 5. HarborHarbor是VMware釋出的一個開源Docker Container Registry，也是屬於CNCF裡的一個專案。Harbor其實並不屬於於PKS產品線的一員，只是通常Pivotal跟VMware會將它與PKS搭配在一起。從上面的架構圖可以看到兩個位於不同地方的Harbor，意思是Harbor有兩種安裝方式：一種是透過PCF Ops Manager安裝，使用Pivotal準備的Harbor Tile去做部署，這樣佈出來的Harbor會在BOSH的管控保護之下。如果不想透過Harbor Tile安裝的話，也可以使用另一種方式自己手動裝(一般是透過docker)，安裝方式可以參考Github)，這兩種方式都是可以的。 Harbor除了能提供一般Container Registry的服務之外，還能夠整合開源專案Notary以及Clair。Notary是CNCF的專案，主要是提供Content Trust簽章服務，確保Image內容不被串改，而Clair能夠針對Image提供靜態分析，根據已知的CVE庫來找出Image淺在的漏洞。 Harbor Tile除了多了一些BOSH Agent以外，其他部分都跟手動裝的版本一模一樣。 6. PKS Client目前PKS以及BOSH都尚未提供GUI介面來做操作，因此都必須透過CLI來做操作。通常會找一台主機安裝所需要的CLI，這台機器我們就稱為PKS Client，PKS Client並沒有限制一定要甚麼作業系統或是多少資源，就只要能安裝CLI以及存取到PKS環境就可以了。 7. NSX-TNSX-T是VMware旗下Networking Virtualization產品，與NSX-V不同，NSX-T是完全獨立的原件，它並沒有限制一定要整合vSphere或是建置在vSphere環境上。像筆者就看過OpenStack整合NSX-T的。另外NSX-T也支援Container環境。 NSX-T在Enterprise PKS環境中同樣不是必須的，也完全與Pivotal無關係(因此沒有甚麼NSX-T Tile xD)，當然你可以不採用NSX-T，直接使用vSphere環境的vSS或是vDS提供給Enterprise PKS環境網路，但是筆者認為PKS要整合NSX-T才能完全增強PKS的功能(可以參照比較表)，因為NSX-T除了提供SDN功能之外，還提供了非常強大的微分割，微分割的單位可以縮小到pod跟pod之間，也就是管理者可以自定義ACL給不同的pod，進而達到強大的隔離性。雖然NSX-T很強大且也可以完全整合PKS，但是筆者認為NSX-T還是一個不太穩定的產品，可能剛出來不久還有許多問題跟功能尚未完成，感覺還有一段路要走。 安裝Enterprise PKS目前安裝方式都是使用PCF Ops Manager來做安裝，由於步驟繁多，因此這裡不多加闡述，有興趣可以參考官方文件 - https://docs.pivotal.io/runtimes/pks/1-4/index.html Preparation: vSphere Environment vCenter Server NSX-T (Optional) Ops Manager OVA PKS Tile Harbor Tile (Optional) BOSH/PKS/Ops Manager CLI PKS支援Cluster 版本Release Notes - https://docs.pivotal.io/runtimes/pks/1-4/release-notes.html#1.4.1 Enterprise PKS目前最新版是1.4.1，此版所對應的Kubernetes版本是v1.13.5，目前尚無法選擇要部署的Kubernetes版本，因為BOSH部署方式是使用VM Template，安裝套件都是已經包好的了，因此如果想要使用新一點版本的Kubernetes，只能等到下次PKS更新之後了。 參考資料 Pivotal.io - https://pivotal.io/ Rancher - https://rancher.com/ OpenShift - https://www.openshift.com/ IBM Cloud Private - https://www.ibm.com/tw-zh/cloud/private BOSH - https://www.bosh.io/docs/ Harbor - https://github.com/goharbor/harbor Notary - https://github.com/theupdateframework/notary Clair - https://github.com/coreos/clair NSX-T Data Center Installation Guide VMware Cloud PKS - https://cloud.vmware.com/vmware-cloud-pks Enterprise PKS document -https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/nsxt_24_install.pdfhttps://docs.pivotal.io/runtimes/pks/1-4/index.html VMware 發佈 VMware Essential PKS -https://blogs.vmware.com/vmware-taiwan/2019/03/01/vmware-%E7%99%BC%E4%BD%88-vmware-essential-pks/ 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Pivotal Container Service</category>
      </categories>
      <tags>
        <tag>pks</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 iThome資安大會筆記]]></title>
    <url>%2F2019%2F2019-iThome-Cyber-Sec-Notes%2F</url>
    <content type="text"><![CDATA[這篇主要是補上今年3月份參加iThome資安大會的一些筆記跟小小心得。今年的資安大會與往年一樣都在台北國際會議中心舉辦，不過由於我第一天有事情，因此今年只有參加到後面兩天，挺可惜的，不過後面兩天也蠻多精彩的演講(雖然大部分還是推銷產品居多)，以下就是我的一些小小筆記，如果想暸解更細部的話，目前簡報已經都釋出在官網上了，可以去參考～ 議程: https://cyber.ithome.com.tw/agenda 3/20Keynote 正面迎戰內部威脅，公司被害還是員工被駭?! 「研發部門是企業命脈，卻最難管理」，鎖太多權限可能導致開發人員沒辦法安裝套件或是一些撰寫的一些程式功能無法執行，但是開發人員有可能會藉由一些方式(如IDE)，使用看起來合法的操作來將企業資料攜出。 為了避免這樣，精品科技開發的產品能夠讓IT人員針對特定IDE去所權限，例如可以設定visual code所執行的CMD沒辦法用來上網或是沒辦法執行敏感指令(copy &amp; paste)，正常情況下的CMD沒事，但是只要是從IDE叫出來的CMD就鎖權限。另外也可擋掉IDE上面的code在執行某些敏感的動作。 除了IDE以外，另外也可以限制如Office這種軟體，權限可以設定的很彈性，例如可以設定只能把文字從外部複製貼上到Word，但是不能從Word把資料複製出來等。 原理推測是透過在端點執行Agent直接去對Windows底層的API去做阻擋，因為不管什麼操作都還是要靠Windows 底層的API來完成，因此直接去鎖底層就可以達到這樣的管理。 Cyberlab 趨勢科技 Target ransom 實機課程，由於禁止將講義跟參考資料帶出，因此只能簡單概述實機內容。 現在勒鎖軟體已經不一定要使用一些非法的程式，許多駭客已經開始使用「正當合法軟體」來作加密的動作(如winrar)，這樣可以避免防毒的偵測。 一般企業中勒鎖病毒都以為只要把中的那台還原或是把資料從備份倒回來就好，但是這樣治標不治本，因為駭客還是存在公司裡，也許他是透過內網裡某台跳板機來攻擊目標，但往往IT人員都只會把焦點放在被攻擊的目標上，以為是那台被駭。要如何追查APT攻擊怎麼造成的對於IT人員來說是非常的困難，透過趨勢科技的APEX產品，就可以幫助IT人員方便的查找問題。 實機操作就是試著扮演駭客透過跳板機去打下AD Server，然後再試著扮演網管人員去追查攻擊流程。 學員扮演角色：駭客 Step 1: 首先連到企業的一個網站 Step 2: 透過聯絡我們上傳照片功能，上傳Webshell到網站。 Step 3: 直接在網站上執行Webshell，新增一個使用者並提權為管理者，然後了解一下Web Server相關資訊(hostname、Web server version…) Step 4: 發現Web Server是Windows Server IIS。 Step 5: 嘗試能不能遠端桌面，不能，可能是防火牆擋住或是沒開RDP功能。 Step 6: 透過Web shell關閉防火牆功能以及開啟RDP Step 7: 在測試一次遠端桌面，成功進到Web Server。 Step 8: 接著要使用procdump跟駭客工具minikatz來取得Web Server上跑的服務的使用者帳密。 Step 9: 由於minikatz會被大部分防毒給擋掉，因此不能直接就把minikatz上傳到Web Server，因此這裡只上傳procdump。 Step 10: procdump是合法的軟體，就是把process dump出來，因此在Web Server上執行沒問題，我們直接將proces資訊dump吃來，然後把Output傳到駭客的電腦。 Step 11: 在駭客電腦執行minikatz分析process dump結果，成功發現kerberos帳號密碼以及服務IP(AD Server)。 Step 12: 測試看看能不能直接透過網路芳鄰輸入IP位置跟帳號密碼連到AD Server的admin$目錄，發現可以成功到admin$目錄。 Step 12: 將預寫好的勒鎖軟體上傳到Web Server，由於是透過winrar去加密程式，因此不怕防毒偵測。 Step 13: 在Web Server執行勒鎖軟體，加密AD Server admin$資料夾。 Step 14: 加密完成，AD Server服務已停止。 學員扮演角色：IT人員 Step 1: 發現AD Server被加密，要開始查找問題是怎麼發生的。 Step 2: 連入趨勢科技的產品Apex Web GUI來查找問題。 Step 3: 首先先隨便在AD Server找一個被加密的檔案名稱，然後透過檔案名稱去搜尋。 Step 4: APEX找出這個檔案存在於哪一台主機(AD Server)，以及他的LOG，透過LOG可以看到他是被什麼Windows指令加密的(rar.exe -df &lt;加密檔名&gt; &lt;原檔名稱&gt; -p&lt;加密的密碼&gt;)還有是哪個使用者帳號執行的、從哪個檔案執行的以及執行時間，看到加密的密碼，網管人員可以松一口氣，表示至少可以還原檔案了。 Step 5: APEX上可以使用LOG產出Root Cause Analysis(RCA)圖，類似關聯圖。關聯圖上面顯示跟那段指令有相關的檔案:PsExec.exe -&gt; encrypt.exe -&gt; cmd.exe -&gt; rar.exe -&gt; &lt;被加密的檔案&gt; 從關聯圖可以看出，有人透過PsExec程式來從別台主機遠端在AD Server上執行encrypt.exe作加密的動作。 Step 6: 接下來要找哪一台主機的log有encrypt.exe這個檔案的紀錄。一樣到APEX GUI透過檔名搜尋 Step 7: 突然搜到除了AD Server以外，還有一台WEB Server一樣有執行這個檔案的紀錄(這個是執行紀錄，就算檔案刪掉也沒用，刪掉的指令也一樣完全都會被記錄進來)，另外還發現是WEB Server奇怪的帳號(剛剛駭客用webshell新增的使用者，一樣駭客刪掉使用者也沒有用，因為一舉一動都被記錄下來)執行encrypt.exe的。 Step 8: 產出RCA圖，可以看到encrypt.exe的原檔名Explorer.exe -&gt; randsom.exe(原檔名) -&gt; PsExec.exe -&gt; encrypt.exe -&gt; cmd.exe -&gt; ... 可以看到Explorer.exe執行randsom.exe，因此可以推斷WEB Server就是源頭也就是跳板機。 Step 9: 回到APEX WEB GUI，以主機為單位，直接去叫出WEB Server上在這段期間執行所有的動作，因此駭客所做的每一步都直接顯示在頁面上(用webshell執行的所有動作)。 Step 10: 結束。 心得:APEX與前面keynote的原理一樣，一樣要在企業的所有機器裝Agent，這個Agent就是趨勢科技的防毒軟體，只是如果要採用agent收集功能，要加授權就可以，這個Agent一樣會去hook windows底層的API，所以每一步驟都完全被記錄下來，駭客要關掉Agent是非常困難的，Agent會用密碼加密，且可能會被設定為唯獨，因此可能不會受勒鎖軟體影響。這個agent會完全沒有隱私，但PC所有的一舉一動都會記錄下來，所以以前常聽到的駭客入侵後要清除足跡，在這種情況下幾乎是完全不管用的，因為就連『清除』這個動作也會被記錄起來。 Android 惡意程式動態解殼研究 一般來說Android開發者會對程式碼進行加殼，來防止反編譯。 加殼方法： 混淆Code(缺點是標準API不能混淆) Dex文件加密 Header修改 Dex data加密 Method動態加解密 VMP (Virtual Protect) 有些開發者會先把code加密，然後故意把不重要的檔案做混淆，讓反編譯的人以為混淆的檔案就是code，然後花心力在處理混淆，殊不知真正的code被加密放在別的地方。 資訊安全中的人工智能對抗 近年來大家很流行使用Machine Learning來training model去對資安攻擊做偵測，但是也衍伸了一種針對Machine Learning Model進行攻擊的「Adversarial Machine Learning」(AML)。 AML是去欺騙Model，使用混淆或是污染的方式來攻擊弄壞Model。例如：Model可以分出狗跟貓，但假如說駭客做一個像貓又像狗的照片(自然界不存在，也就是非正常的Data)，然後餵給Model，Model可能就會造成誤判。另外一種方式是圖片是狗，但是某一個pixel故意放烤麵包機，以人的肉眼來看看不出來，但是Model在判別時，就會造成錯誤，可能會誤判。 除圖像的以外，垃圾郵件的偵測也可以使用AML，例如在垃圾郵件裡面故意塞一段正常文字，Model可能就會以為這是正常的郵件。或是郵件裡面的某些文字故意換成同義詞，這樣可能也可以繞過垃圾郵件判別。 絕大部分的Model都不太可能百分之百不被騙或是混淆，如果一個Model完全不會被混淆，那個Model也沒有AML的價值。 AML攻擊手法可以分成兩種： Evasion Attack: 製作特製的data來騙或繞過Model，又可分成黑箱跟白箱，黑箱是駭客不知道Model長怎樣，只能不斷透過Output去猜測，白箱是指駭客已經事先了解Model規則。 Poision Attack: 去破壞Model，例如有些chat bot採用online traing機制，一邊聊天一邊學，但只要一直餵給他特定的data，就可以讓Model壞掉，學奇怪的東西或是誤判。 防禦方式： Evasion Attack: 駭客一定要有方法去試Model Output，因此只要偵測是否有人在短時間內大量測試或是人工簡單看一下是否有不尋常非自然得data，如果有就BAN掉。 Model retrain：一但發現Model分析資料不準，就馬上修正Model並重新training。retrain又可分成主動式跟被動式，主動式是企業內部red/blue team主動去攻擊Model，然後修正Model。被動是等被攻擊了，才Retrain。 駭客如何利用公開工具在內部網路中暢行無阻iThome報導 -https://www.ithome.com.tw/news/129486?fbclid=IwAR2rBpZx6r-fWXgmsTupgUrzkWsOiCunvCleq5oyfONtz6zyd3aQ1Zr1Abg 紅隊演練 vs 滲透測試 紅隊演練：駭客思維出發、全面且廣泛、目的是竊取企業資料 滲透測試：在特定範圍、針對特定目標、目的是挖掘系統漏洞 攻擊鍊: 目標偵查 -&gt; 武器研製 -&gt; 發動攻擊 -&gt; 維持控制 -&gt; 內網滲透 -&gt; 偷取資料 目標偵查: Recon-Ng工具 武器研製: Metasploit(產生惡意巨集文件) 維持控制：安裝後門 內網滲透： BloodHound(能夠將AD Server網域內的元件以圖性化顯示並且提供攻擊路徑)、Responder(針對LLMNR或NBT-NS,MDNS謝進行poisoning attack) 偷取資料：dnscat2(DNS Tunnel)、icmpsh(ICMP Tunnel)。 防禦方對策 研究公開工具，將其特徵加入資安產品 禁止不安全的巨集或是不常見的腳本運行(hta,vbs…)。 使用公開工具掃網路環境開啟服務並關閉未使用的功能。 檢視不安全的ACL配置 檢視DNS伺服器的解析方式 3/21KEYNOTE【Live Demo】工業控制 Testbed 與資安攻防研究 近幾年工業製造的資安問題也慢慢開始被重視。 講者現場Demo一個模擬火車環境，駭客入侵鐵道管理系統，然後將火車引導到斷軌的地方。 攻擊手法為： 滲透至內網-&gt; 中間人攻擊 -&gt; 封包側錄或解析 -&gt; 串改封包或是發放假封包給控制端 台灣工控設備有10546個暴露於網際網路(具有真實調查資料)，且大部分都是走HTTP協定，非常不安全，暴露的產業有能源業、大專院校、政府機關、停車場業者、鋼鐵製造業、飯店大夏管理等。 工業控制網路防護自我健康檢查 資產盤點： 是否已辨識關鍵基礎設施？ 是否進行風險管理與評估？ 使否檢視網段劃分與工業控制網路邊界？ 是否檢視網路架構設計邏輯？ ICS系統帳號管理： 是否制定帳號鎖定、管理政策？ 是否查找、刪除任何預設的特權帳號及密碼？ 是否要求使用強密碼和多因素身份驗證？ 是否限制特權帳號數量？尤其第三方供應商 是否要求OT場域人員定期更換密碼 OT網路隔離性(ioslate)檢視 是否可以直接由網際網路存取ICS？ 是否可以直接由IT網路存取ICS? 是否有嚴謹之傘端存取管理政策？ 防火牆是否已隔離所有需保護的ICS網路和設備。 KEYNOTE善用駭客思維：如何聯合白帽駭客共同加強資安防禦能量 紅隊演練不應該事先告知，且紅隊會一直學習新的攻擊技術，增強攻擊能力，這樣可以考驗藍隊的應對處理機制。 企業產品安全保護方式(按照順序)： Code &amp; architecture review: 針對工程師或是員工上風險評估以及資訊安全概念的相關課程。工程師內部針對code去做review，看有沒有安全風險。 動＆靜態程式測試 紅隊演練：企業內部團隊找漏洞。 開啟Bug Bounty：讓全世界的白帽駭客一起來找漏洞。 紅隊演練的好處： 能夠讓企業了解產品的弱點及漏洞面向。 能夠讓藍隊增強企業安全的防守。 允許管理層衡量安全解決方案的投資。 能夠增加攻擊的成本，讓駭客攻擊更加困難。 資安脫口秀 甲方不應該看哪家廠商設備數字或成績漂亮就選擇他，應該是根據自身需要來選擇，可以整理近幾年公司遇到的資安事件，然後再跟乙方做討論，這樣才能選擇到公司可用的解決方案。 突破困境：資安開源工具之應用分享 簡報 -https://www.slideshare.net/jasoncheng7115/20190321-137504140?fbclid=IwAR26IO1-XOw_0xPIX2jPZB859H2B9XC8Xt35WxO0EZVvx9yEW6ZL7TTYST8 開源工具 解決方案 Proxmox 開源伺服器虛擬化管理平台 LibreNMS 開源裝置與服務狀態監視 Open-audIT 開源IT資產管理系統 Graylog 開源事件記錄管理與分析 FreeNAS 開源儲存伺服器系統 Duplicati 開源資料備份系統 PacketFence 開源網路存取控制系統 WSO2 IoT 開源行動裝置管理(MDM BYOD管理) OpenVAS 開源弱點檢測管理平台 MobSF 開源APP安全測試平台 SonarQube 開源程式碼檢測平台 Proxmox MG 開源郵件閘道伺服器 開源套件選擇要點 版本更新: 可以查看更新日期 活躍程度：可以知道這個專案到底有沒有在維護 商業支援：是否有付費版跟開源版，通常有付費版撐腰的比較好。 留意授權問題(GPL,BSD, Apache…) Open Hub - 開源專案安全檢測與授權建議 解密 HTTPS 雲端服務的美麗與哀愁: 側錄 / 備份 / 防毒 / 稽核 TLS提供了實質性的好處，但是卻也增加了分析流量的困難。 部分國家是禁止對HTTPS做解密，因為會有隱私權問題。 通常金融業是不能對HTTPS做解密，因為資料過度敏感。 有些軟體例如Line或是Messenger，沒辦法做解密，因為這些程式只要發現憑證被改就會被擋掉。 心得: 大部分的HTTPS解密都是透過proxy來做到，client端裝agent來換掉憑證，然後proxy端做解密的動作，接著再由proxy去跟Web Server去做連線，這樣就可以取得HTTPS的內容。 廠商(只有部分)郵件過濾及郵件安全： Proofpoint 郵件伺服器及郵件安全 Openfind 特權帳號監控管理 Cyberark HTTPS流量管理監控(解密HTTPS) L7Ntwork F5 雲端安全 Tufin F-Secure 透過DNS來防護 Cisco Umbrella 端點檔案加密(為重要檔案加密) 中華電信SecurBox DDoS防護 威睿科技 GenieATM(透過設備將流量導到F5那些去做清洗，他們只做導向的動作而已) Arbor Akamai Log管理 ALog 整合型安全風險管理平台 tenable IBM Security Birdman新創公司 CYCARRIER 其餘學習 以往對付DDoS，廠商都是直接放一台自家設備在Gateway，當攻擊發生時設備將流量導向到Cloud Scrubbing Center去做清洗，但是這樣子對Client端還是有很大得負擔，因此發現今年許多處理DDoS的廠商，在Redirect的部分改用了BGP forwarding或是DNS Forwarding，直接在源頭就將流量導向到Scrubbing Center，這樣就不會增加Client的負擔。 市面上大部分的IP Cam都是換湯不換藥，外殼是自己設計，但是晶片可能就採用大陸晶片，這種認證通常都很脆弱，只要使用一些現有的APP就可以掃到IP cam然後免認證登入，因此在買IP cam的時候，最好選擇有通過『 IP Camera 場域實驗室』認證的IP Cam。 電影裡騙過管理者IP CAM畫面現實是做得到的，只要透過arp spoofing先去欺騙IP CAM管理器，這樣管理器就會以為駭客的電腦才是IP CAM，然後駭客再送假訊號(畫面正常的影片)給管理器，這樣管理器那邊就會一直看到是正常的畫面。 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Security</category>
      </categories>
      <tags>
        <tag>conference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH Tunnel介紹]]></title>
    <url>%2F2019%2FSSH-Tunnel-Introduction%2F</url>
    <content type="text"><![CDATA[由於最近碰到一些特殊網路情境必須使用到SSH Tunnel，因此花點時間複習了一下。SSH Tunnel是個非常好用的技術，可以在一些比較安全或封閉的網路將流量透過SSH Protocol傳輸出去。使用SSH Tunnel一定要準備一個與Client端不同網路的Remote Server當作跳板機，其原理類似Port Forwarding的概念，在Client以及Remote Server之間建立一個SSH Tunnel，然後將特定的服務全部透過這個Tunnel來傳送，就可以達到加密及穿透的效果。 SSH Tunnel又可分成兩種: Local Tunnel (又稱SSH Local Forwarding) Remote Tunnel (又稱SSH Remote Forwarding) 這兩種差別其實就是要從哪一端將流量透過SSH Tunnel傳送出去，一個是從local端傳送給Remote端，一個是從Remote傳回來給local。接下來會針對這兩種Tunnel做細部解釋以及適用的情境。 Local TunnelLocal Tunnel主要是從Local端(Client端)，將Client端特定服務的流量透過SSH傳給Remote Server，再透過Remote Server傳出去給Application Server。其使用的情境可以分成兩種：一種是想要連到特定的服務，但是卻不想被網管監測到流量，就可以使用SSH來傳送流量出去，這樣網管只能看到SSH加密的流量，沒辦法看到內容。另一種是公司防火牆有限制員工不能連到特定服務或網站，也可以將流量包到SSH裡面來繞過防火牆出去(前提是防火牆沒有檔SSH服務)。 指令(此指令需在Client端執行)：$ ssh -NfL &lt;local listening IP&gt;:&lt;local listening port&gt;:&lt;application ip&gt;:&lt;application port&gt; &lt;Remote Server&gt;# Example$ ssh -NfL localhost:1234:&lt;web server ip&gt;:80 user@&lt;remote server ip&gt; -N 代表不要開起shell code模式-f 代表在背景執行-L 代表Local Tunnel&lt;local listening IP&gt; 要在Client上監聽的IP。&lt;local listening port&gt; 要在Client上監聽的Port，可自行定義。&lt;application ip&gt; 要存取的服務IP&lt;application port&gt; 要存取的服務port 這邊意思是假如Client上有任何Request要傳送到&lt;local listening IP&gt;:&lt;local listening port&gt;，Client就會將他轉送到SSH Tunnel來傳給Remote Server，Remote Server收到流量後，會把封包轉送到&lt;application ip&gt;:&lt;application port&gt;，因此從Client端到Remote Server端之間的路是透過SSH傳送的(加密連線)。Client &lt;--SSH Tunnel--&gt; Remote Server &lt;--Internet--&gt; Application Server 請注意這邊的&lt;local listening port&gt;跟&lt;application port&gt;與SSH服務使用到的Port完全沒有任何關係，預設SSH Server使用的Port還是22，然後Client端是亂數產生。如果Remote Server的SSH Port不是22的話，那就必須在指令後面加上 -p &lt;port number&gt; Example:ssh -NfL localhost:1234:&lt;web server ip&gt;:80 user@&lt;remote server ip&gt; -p 3000 這邊需要注意的是&lt;local listening IP&gt;可以打也可以不打，預設是localhost(127.0.0.1)，如果要監聽在不同的IP，需要針對sshd_config做設定，在稍後會做說明。 情境1想要在公司使用ptt(ssh bbsu@ptt.cc)，但是不想被網管發現。角色: Client: 公司個人電腦or筆電，需要可以使用SSH指令。 Remote Server: 當作跳板的機器，需要有SSH Server服務，且必須位於公司網路之外。Client必須連得到它。 ptt.cc: 要連線的Application Server。 Step 在Client上Console執行Local Tunnel。 $ ssh -NfL 1234:ppt.cc:23 &lt;Remote Server&gt; 在Client端檢查是否有成功建立起Tunnel $ netstat -plnt $ ps -ef | grep ssh 接下來就可以透過Client連到ptt $ ssh bbsu@localhost -p 1234 如果要關掉Tunnel，只需要在Client上面執行以下指令： $ ps -ef | grep ssh$ kill &lt;ssh tunnel 那條服務的PID&gt; 說明在這個情境中，當Client收到要通往localhost:1234的Request時，就會將它丟到SSH Tunnel傳送給Remote Server，Remote Server收到後再把它丟到ptt.cc:22，當封包回來也是一樣，只是就反過來，Remote Sever將Response透過SSH Tunnel丟回給Client，Client再將封包從SSH Tunnel丟給1234 port。 情境2公司防火牆擋掉xxx網站，要如何才能繞過防火牆連到網站角色: Client: 公司個人電腦or筆電，需要可以使用SSH指令。 Remote Server: 當作跳板的機器，需要有SSH Server服務，且必須位於公司網路之外，Client必須連得到它。 Web Server: 要連線的Application Server。 Step 在Client上Console執行Local Tunnel。 $ ssh -NfL 80:&lt;web server&gt;:80 &lt;Remote Server User&gt;@&lt;Remote Server IP&gt; 在Client端檢查是否有成功建立起Tunnel $ netstat -plnt $ ps -ef | grep ssh 接下來就可以透過Client上的瀏覽器連到網站(輸入http://localhost) #除了瀏覽器之外，也可以使用curl測試$ curl http://localhost 如果要關掉Tunnel，只需要在Client上面執行以下指令： $ ps -ef | grep ssh$ kill &lt;ssh tunnel 那條服務的PID&gt; 說明在這個情境中，當Client收到要通往localhost:80的Request時，就會將它丟到SSH Tunnel傳送給Remote Server，Remote Server收到後再把它丟到&lt;web server&gt;:80，封包回來得時候也是一樣，只是順序反過來。 這邊需要注意的是，如果是要Redirect 443 port，可能會因為憑證被換掉而導致憑證錯誤，因此這種情境不太建議使用在有TLS加密的Web Server，純粹拿來測試就好。另外如果Web Server是在Load Balancer後面，那這樣可能也會導致Local Tunnel無法成功，這邊不太確定為什麼，有可能是因為導到後面後，IP就不是原本設定Tunnel的那個Web Server IP，因此流量就不會正常的走SSH Tunnel。 Remote TunnelRemote Tunnel是我比較常用的Tunnel方式，因為可以達到類似於VPN的功能，來存取內網的服務，可以應用在很多種情境上。Remote Tunnel是指從Remote端(Remote Server端)，將特定服務的流量透過SSH傳回給Client端上的服務。 指令(此指令需在Client上執行)：$ ssh -NfR &lt;remote listening IP&gt;:&lt;remote listening port&gt;:&lt;client listening ip&gt;:&lt;client listening port&gt; &lt;Remote Server&gt;# Example$ ssh -NfR localhost:1234:localhost:80 user@&lt;remote server ip&gt; -N 代表不要開起shell code模式-f 代表在背景執行-R 代表Remote Tunnel&lt;remote listening IP&gt; 要在Remote Server上監聽的IP&lt;remote listening port&gt; 要在Remote Server上監聽的Port，可自行定義。&lt;client listening ip&gt; Client上面服務監聽的IP&lt;client listening port&gt; Client上面服務port 這段指令意思是任何送到Remote Server上&lt;remote listening IP&gt;:&lt;remote listening port&gt;的Request，Remote Server都會將他透過SSH Tunnel傳送到給Client，Client再將Request丟給自己的&lt;client listening ip&gt;:&lt;client listening port&gt;服務。外部主機 --SSH--&gt; Remote Server &lt;--SSH Tunnel--&gt; Client 請注意這邊的&lt;remote listening port&gt;跟&lt;client listening port&gt;與SSH服務使用到的Port完全沒有任何關係，預設SSH Server使用的Port還是22，然後Client端是亂數產生。如果Remote Server的SSH Port不是22的話，那就必須在指令後面加上 -p &lt;port number&gt; Example: ssh -NfR localhost:1234:localhost:80 user@&lt;remote server ip&gt; -p 3000 這邊需要注意的是&lt;remote listening IP&gt;可以打也可以不打，預設是localhost(127.0.0.1)，如果要監聽在不同的IP，需要針對sshd_config做設定，在稍後會做說明。 一般情境公司的PC是在內網裡面，且是虛擬IP，有沒有辦法不透過VPN，從公司外面SSH連回來自己的PC？or公司的防火牆擋掉外對內的連線，因此沒辦法從外部透過SSH連回公司裡自己建置的SSH Server，是否有辦法不透過VPN，從公司外面SSH連回來SSH Server？ 角色: Client: 公司內部的主機，也就是希望能從公司外部透過SSH連回的SSH Server，需要可以使用SSH指令。 Remote Server: 當作跳板的機器，需要有SSH Server服務，且必須位於公司網路之外，Client要能連得到它。 外部電腦: 位於公司外部的電腦，必須能連得到Remote Server。 Step 在Client端執行Remote SSH Tunnel。 $ ssh -NfR 1234:localhost:22 &lt;Remote Server User&gt;@&lt;Remote Server IP&gt; 在Remote Server端以及Client端檢查是否有成功建立起Tunnel Client ps -ef | grep ssh Remote Server netstat -plnt 接下來先透過外部電腦連到Remote Server。 # 在外部電腦上執行$ ssh &lt;Remote Server User&gt;@&lt;Remote Server IP&gt; 透過Remote Server反向連回Client。 # 在Remote Server上執行$ ssh localhost -p 1234 如果要關掉Tunnel，只需要在Client上面執行以下指令： $ ps -ef | grep ssh$ kill &lt;ssh tunnel 那條服務的PID&gt; 說明在這個情境中，主要是先在Client開啟Remote Tunnel，然後Remote Tunnel會在Remote Server上監聽1234 port，當有Request從1234 port來的話，Remote Server就會透過SSH轉送回Client，Client收到後再將這個Request轉送給自己的22 Port。 設定讓外部能存取SSH Tunnel預設SSH Tunnel是不能監聽在localhost以外的IP，因此每次使用Remote Tunnel就一定要先連到Remote Server才能反連回去，這樣不太方便，因此我們可以在SSH Server上面允許SSH Tunnel可以監聽在其他Port。 到Remote Server上開啟sshd_config $ vim /etc/ssh/sshd_config 找到GatewayPorts選項並設成yes，如果沒有這個選項可以自己加上去。 GatewayPorts yes 重啟SSH服務 $ systemctl restart sshd.service 這樣就可以將SSH Tunnel監聽在任何IP上 # Local Tunnel$ ssh -NfL 0.0.0.0:1234:&lt;web server ip&gt;:80 user@&lt;remote server ip&gt;$ ssh -NfL xx.xx.xx.xx:1234:&lt;web server ip&gt;:80 user@&lt;remote server ip&gt;...# Remote Tunnel$ ssh -NfR 0.0.0.0:1234:localhost:80 user@&lt;remote server ip&gt;$ ssh -NfR xx.xx.xx.xx:1234:localhost:80 user@&lt;remote server ip&gt;... 設定外部能存取後，就可以結合各式各樣的應用，例如： Reverse Proxy透過Remote Server反向連回公司PC上的Web Server。$ ssh -NfR 0.0.0.0:80:localhost:80 user@&lt;remote server ip&gt; 如此一來不需要連到Remote Server，只要在任何能存取到Remote Server的主機使用瀏覽器連http://&lt;remote server ip&gt;或是執行curl http://&lt;remote server ip&gt;就可以連到公司的web server。 RDP透過Remote Server反向連回公司PC上的遠端桌面服務。$ ssh -NfR 0.0.0.0:3389:localhost:3389 user@&lt;remote server ip&gt; 如此一來不需要連到Remote Server，只要在任何能存取到Remote Server的主機上使用遠端桌面連到&lt;remote server ip&gt;:3389，就可反向連到公司PC上的遠端桌面服務。 這邊都只有使用Remote Tunnel來當作範例，但是其實Local Tunnel也可以把&lt;local listening IP&gt;設定成監聽任何IP，只是目前比較少碰到需要把Local Tunnel開放外部存取這種情況，因此這裡就不特別闡述，有興趣可以自行測試看看。 如何防止SSH TunnelSSH Tunnel防範的方法有很多，這邊目前先列出幾種比較常聽到的，之後如果還有學習到別的方法再補充上來。 SSH Server關閉SSH Tunnel服務，開啟sshd_config設定AllowTcpForwarding no。這樣可以防止別人用你的SSH Server當SSH Tunnel跳板機。 在防火牆直接擋掉SSH服務，不管內對外，外對內都擋。(曾經真的有遇到類似的案例，只是是限制特定的主機，例如給guest使用的或是一些非資訊技術相關的End User用的，並不是真的全公司的主機都擋)。 使用一些端點安全防護的產品直接禁止客戶使用SSH Tunnel指令或是SIEM產品去trace哪個使用者使用了Tunnel。 參考資料 SSH Tunnel - Local and Remote Port Forwarding Explained With Examples - https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html How to make ssh tunnel open to public? - https://superuser.com/questions/588591/how-to-make-ssh-tunnel-open-to-public Securing Your SSH Server - http://download.asperasoft.com/download/docs/proxy/1.4.0/admin_linux/webhelp/dita/securing_ssh_server.html 文章內容的轉載、重製、發佈，請註明出處: https://pohsienshih.github.io]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
</search>
